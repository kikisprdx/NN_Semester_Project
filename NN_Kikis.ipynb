{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62cc135-4df5-4dff-85c0-3f170318b51c",
   "metadata": {},
   "source": [
    "# Japanese Vowel Speaker Recognition Project For Nueral Networks Course\n",
    "\n",
    "### Project description : \n",
    "\n",
    "The goal of this project is to develop a 9-class classifier that can accurately identify Japanese male speakers based on short spectral recordings of their vowel utterances. \n",
    "\n",
    "The task involves training the classifier using a dataset of 12-channel time series data representing vocal samples, and then evaluating its performance on a separate test set to hopefully have achieved the lowest possible misclassification rate.\n",
    "\n",
    "### Data overview :\n",
    "\n",
    "The dataset consists of 270 training and 370 test recordings of 12-channel time series data representing spectral recordings of the Japanese vowel /ae/ uttered by 9 male speakers, with each recording varying in length from 7 to 29 timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "eadea309-4544-491c-8254-9bdb6b63f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e351c-ed64-4297-83f2-8a452e34d183",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "This step involves reading the data from external sources and converting it into a format that can be used by the machine learning algorithm.\n",
    "\n",
    "Specifically, it's reading the raw data from text files provided to us, structuring it into input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "48dec52c-c79b-4ebd-98a8-719957c64380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(filename):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        current_input = []\n",
    "        # Line by line we strip and split all values \n",
    "        for line in lines:\n",
    "            values = line.strip().split()\n",
    "            # If not the end of a record \n",
    "            if values and values[0] != '1.0' and values[0]!='1.00':\n",
    "                # We add the whole line (if invalid its going to be set to 0) \n",
    "                # First 12 set of values are input, the next 12 are output\n",
    "                # I only got this by looking through the original matlab file and inferencing this fact\n",
    "                # I could be wrong so maybe I'll ask the professor \n",
    "                input_values = [float(val) if val else 0 for val in values]\n",
    "                current_input.append(input_values)\n",
    "            # We're at the end\n",
    "            elif values and values[0] == '1.0' and values[0]!='1.00':\n",
    "                inputs.append(current_input)\n",
    "                current_input = []\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Read the files\n",
    "train_inputs = read_txt_file('ae.train')\n",
    "test_inputs = read_txt_file('ae.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89aa4c-0f86-4b57-9c8c-033af4974c61",
   "metadata": {},
   "source": [
    "### One-hot Encoding \n",
    "\n",
    "Here we are creating corresponding one-hot encoded output labels for both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "58f17882-7f38-4e02-b451-3d4db86fe86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = []\n",
    "for i in range(270):\n",
    "    speaker_index = (i // 30) + 1  # Assuming 9 speakers, each with 30 time series\n",
    "    l = len(train_inputs[i])\n",
    "    teacher = np.zeros((l, 9))\n",
    "    teacher[:, speaker_index - 1] = 1  # One-hot encoding for speaker index\n",
    "    train_outputs.append(teacher)\n",
    "\n",
    "# Create teacher signals for test data\n",
    "test_outputs = []\n",
    "speaker_index = 1\n",
    "block_counter = 0\n",
    "block_lengths = [31, 35, 88, 44, 29, 24, 40, 50, 29]  # Assuming the same block lengths as in MATLAB code\n",
    "for i in range(370):\n",
    "    block_counter += 1\n",
    "    if block_counter > block_lengths[speaker_index - 1]:\n",
    "        speaker_index += 1\n",
    "        block_counter = 1\n",
    "    l = len(test_inputs[i])\n",
    "    teacher = np.zeros((l, 9))\n",
    "    teacher[:, speaker_index - 1] = 1  # One-hot encoding for speaker index\n",
    "    test_outputs.append(teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293168e-3324-42c4-95a9-eb39f4f9ef39",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "We find the maximum length of input and output sequences in both training and test sets, then pad all sequences to these maximum lengths with zeros, ensuring uniform dimensions for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "84195c87-08df-4463-8bd2-88f866566f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each recording of each input/output dataset pair, record the maximum length of a recording\n",
    "max_len_train_inputs = max(len(ts) for ts in train_inputs)\n",
    "max_len_train_outputs = max(len(ts) for ts in train_outputs)\n",
    "max_len_test_inputs = max(len(ts) for ts in test_inputs)\n",
    "max_len_test_outputs = max(len(ts) for ts in test_outputs)\n",
    "\n",
    "# Pad all recordings with 0s to reach max_len...\n",
    "train_inputs = [np.pad(ts, ((0, max_len_train_inputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in train_inputs]\n",
    "train_outputs = [np.pad(ts, ((0, max_len_train_outputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in train_outputs]\n",
    "test_inputs = [np.pad(ts, ((0, max_len_test_inputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in test_inputs]\n",
    "test_outputs = [np.pad(ts, ((0, max_len_test_outputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in test_outputs]\n",
    "\n",
    "# Convert to Numpy arrays for fun and easy manipulation\n",
    "train_inputs = np.array(train_inputs)\n",
    "test_inputs = np.array(test_inputs)\n",
    "train_outputs = np.array(train_outputs)\n",
    "test_outputs = np.array(test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd8fbc-6bd6-400a-a64d-1cf8feea8810",
   "metadata": {},
   "source": [
    "\n",
    "Here we inspect the dimensions of the data to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "e12961b3-57fa-44e1-b248-f6a2de78360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 26, 12)\n",
      "(370, 29, 12)\n",
      "(270, 26, 9)\n",
      "(370, 29, 9)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shapes here\n",
    "## TRAINING\n",
    "# 270 -> Training Recordings -> 26 is the longest recording (time step) -> 12 is the number of features vectors \n",
    "# => 3D array of all recordings, with each of their time steps, with each of the 12 features present \n",
    "\n",
    "## TEST\n",
    "# 370 -> Test recordings -> 29 is the longest recording -> 9 is the number of output vectors (9 speakers) \n",
    "# => 3D array of all recording, with each of their time steps, with a speaker column corresponding to each timestep\n",
    "print(train_inputs.shape)\n",
    "print(test_inputs.shape)\n",
    "print(train_outputs.shape)\n",
    "print(test_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6f276-e043-482f-9812-f5e5577f4036",
   "metadata": {},
   "source": [
    "### Data dimension transformation\n",
    "\n",
    "Here we reshape the input and output data for both training and testing sets from 3D arrays into 2D pandas DataFrames, flattening the time series dimension and adding index columns for time series and time step.\n",
    "\n",
    "This creates structured DataFrames suitable for our machine learning model that expects a 2D input, while still preserving the original time series structure through multi-level indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "f3f7e18f-87e7-48d5-98d1-57db5873692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Datasets\n",
    "\n",
    "def flatten_data_sets(inputs, outputs):\n",
    "    ## Flatten 'recordings' and 'time-steps' in a single dimension, while perserving the other vectors. \n",
    "    ## The -1 is just a pythonic way of telling the function to calculate the size of the flattened first dimension automatically\n",
    "    train_inputs_2d = inputs.reshape(-1, 12)  \n",
    "    \n",
    "    ## Then we create a pandas Dataframe, and label each feature \n",
    "    train_inputs_df = pd.DataFrame(train_inputs_2d, columns=[f'feature_{i}' for i in range(1, 13)])\n",
    "    \n",
    "    # Create a timeseries column that corresponds what recording each row belongs to,\n",
    "    # Where inputs.shape[0] is the first dimension (270), inputs.shape[1] is the length of each recording (26)\n",
    "    # Then using np.arrange(inputs.shape[1]) creates an array from 0 to 270 (evenly spaced indicies). \n",
    "    # Then np.repeat( ... , inputs.shape[1])\n",
    "    # Repeats for each time of shape[0] (270), that index shape[1] times. \n",
    "    # This results in a column where each index is associated with a recording\n",
    "    # lasting for the recording's timestep length. Effectively labeling what's going on\n",
    "    train_inputs_df['time_series'] = np.repeat(np.arange(inputs.shape[0]), inputs.shape[1])\n",
    "    \n",
    "    # This almost does the same thing but np.tile just copies the array (inputs.shape[1] for shape[0] times), resulting \n",
    "    # in a proper timestep series column\n",
    "    train_inputs_df['time_step'] = np.tile(np.arange(inputs.shape[1]), inputs.shape[0])\n",
    "    # This is just an easy way to ensure the columns are at the front\n",
    "    train_inputs_df_X = train_inputs_df.set_index(['time_series', 'time_step'])\n",
    "    train_inputs_df_X = train_inputs_df_X.reset_index()\n",
    "\n",
    "    # The exact same is done for the outputs, but instead of features, we're dealing with speakers\n",
    "    train_outputs_2d = outputs.reshape(-1, 9)\n",
    "    train_outputs_df = pd.DataFrame(train_outputs_2d, columns=[f'speaker_{i}' for i in range(1, 10)])\n",
    "    train_outputs_df['time_series'] = np.repeat(np.arange(outputs.shape[0]), outputs.shape[1])\n",
    "    train_outputs_df['time_step'] = np.tile(np.arange(outputs.shape[1]), outputs.shape[0])\n",
    "    train_outputs_df_Y = train_outputs_df.set_index(['time_series', 'time_step'])\n",
    "    train_outputs_df_Y = train_outputs_df_Y.reset_index()\n",
    "    return train_inputs_df_X, train_outputs_df_Y\n",
    "\n",
    "train_inputs_df_X, train_outputs_df_Y = flatten_data_sets(train_inputs, train_outputs)\n",
    "test_inputs_df_X, test_outputs_df_Y = flatten_data_sets(test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "a4b2aeb5-6b11-48e0-872b-f1d0a0864ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10730, 14)\n",
      "(7020, 14)\n"
     ]
    }
   ],
   "source": [
    "# Validate shapes\n",
    "assert test_inputs_df_X.shape == (10730, 14), f\"Expected shape of (10730, 14), but got {test_inputs_df_X.shape}\"\n",
    "assert train_inputs_df_X.shape == (7020, 14), f\"Expected shape of (7020, 14), but got {train_inputs_df_X.shape}\"\n",
    "\n",
    "print(test_inputs_df_X.shape) \n",
    "print(train_inputs_df_X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "1a9963cb-d5a0-4ccb-b485-025107c94093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10730, 11)\n",
      "(7020, 11)\n"
     ]
    }
   ],
   "source": [
    "assert test_outputs_df_Y.shape == (10730, 11), f\"Expected shape of (10730, 14), but got {test_outputs_df_Y.shape}\"\n",
    "assert train_outputs_df_Y.shape == (7020, 11), f\"Expected shape of (7020, 14), but got {train_outputs_df_Y.shape}\"\n",
    "\n",
    "print(test_outputs_df_Y.shape)\n",
    "print(train_outputs_df_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "27498686-5437-4ec2-8145-8fc16740166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_series</th>\n",
       "      <th>time_step</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.860936</td>\n",
       "      <td>-0.207383</td>\n",
       "      <td>0.261557</td>\n",
       "      <td>-0.214562</td>\n",
       "      <td>-0.171253</td>\n",
       "      <td>-0.118167</td>\n",
       "      <td>-0.277557</td>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.126701</td>\n",
       "      <td>-0.306756</td>\n",
       "      <td>-0.213076</td>\n",
       "      <td>0.088728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.891651</td>\n",
       "      <td>-0.193249</td>\n",
       "      <td>0.235363</td>\n",
       "      <td>-0.249118</td>\n",
       "      <td>-0.112890</td>\n",
       "      <td>-0.112238</td>\n",
       "      <td>-0.311997</td>\n",
       "      <td>-0.027122</td>\n",
       "      <td>0.171457</td>\n",
       "      <td>-0.289431</td>\n",
       "      <td>-0.247722</td>\n",
       "      <td>0.093011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.939205</td>\n",
       "      <td>-0.239664</td>\n",
       "      <td>0.258561</td>\n",
       "      <td>-0.291458</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.102034</td>\n",
       "      <td>-0.383300</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.169510</td>\n",
       "      <td>-0.314894</td>\n",
       "      <td>-0.227908</td>\n",
       "      <td>0.074638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.717517</td>\n",
       "      <td>-0.218572</td>\n",
       "      <td>0.217119</td>\n",
       "      <td>-0.228186</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>-0.137624</td>\n",
       "      <td>-0.403318</td>\n",
       "      <td>-0.009643</td>\n",
       "      <td>0.164607</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>-0.210105</td>\n",
       "      <td>0.098098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.741191</td>\n",
       "      <td>-0.279891</td>\n",
       "      <td>0.196583</td>\n",
       "      <td>-0.236377</td>\n",
       "      <td>-0.032012</td>\n",
       "      <td>-0.090612</td>\n",
       "      <td>-0.363134</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>0.124298</td>\n",
       "      <td>-0.351171</td>\n",
       "      <td>-0.216545</td>\n",
       "      <td>0.113899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_series  time_step  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0            0          0   1.860936  -0.207383   0.261557  -0.214562   \n",
       "1            0          1   1.891651  -0.193249   0.235363  -0.249118   \n",
       "2            0          2   1.939205  -0.239664   0.258561  -0.291458   \n",
       "3            0          3   1.717517  -0.218572   0.217119  -0.228186   \n",
       "4            0          4   1.741191  -0.279891   0.196583  -0.236377   \n",
       "\n",
       "   feature_5  feature_6  feature_7  feature_8  feature_9  feature_10  \\\n",
       "0  -0.171253  -0.118167  -0.277557   0.025668   0.126701   -0.306756   \n",
       "1  -0.112890  -0.112238  -0.311997  -0.027122   0.171457   -0.289431   \n",
       "2  -0.041053  -0.102034  -0.383300   0.019013   0.169510   -0.314894   \n",
       "3  -0.018608  -0.137624  -0.403318  -0.009643   0.164607   -0.323267   \n",
       "4  -0.032012  -0.090612  -0.363134  -0.012571   0.124298   -0.351171   \n",
       "\n",
       "   feature_11  feature_12  \n",
       "0   -0.213076    0.088728  \n",
       "1   -0.247722    0.093011  \n",
       "2   -0.227908    0.074638  \n",
       "3   -0.210105    0.098098  \n",
       "4   -0.216545    0.113899  "
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs_df_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "7706ca72-5241-4186-93c1-cddceb0042d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_series</th>\n",
       "      <th>time_step</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_2</th>\n",
       "      <th>speaker_3</th>\n",
       "      <th>speaker_4</th>\n",
       "      <th>speaker_5</th>\n",
       "      <th>speaker_6</th>\n",
       "      <th>speaker_7</th>\n",
       "      <th>speaker_8</th>\n",
       "      <th>speaker_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_series  time_step  speaker_1  speaker_2  speaker_3  speaker_4  \\\n",
       "0            0          0        1.0        0.0        0.0        0.0   \n",
       "1            0          1        1.0        0.0        0.0        0.0   \n",
       "2            0          2        1.0        0.0        0.0        0.0   \n",
       "3            0          3        1.0        0.0        0.0        0.0   \n",
       "4            0          4        1.0        0.0        0.0        0.0   \n",
       "\n",
       "   speaker_5  speaker_6  speaker_7  speaker_8  speaker_9  \n",
       "0        0.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs_df_Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "15eff7e0-4b8b-4866-802d-fe39e335e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF for inspection\n",
    "train_inputs_df_X.to_csv('train_inputs.csv')\n",
    "train_outputs_df_Y.to_csv('train_outputs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca306e4-7ece-43a5-b936-12ff76da847b",
   "metadata": {},
   "source": [
    "### Tensor Conversion\n",
    "\n",
    "We save the training input and output DataFrames to CSV files.\n",
    "\n",
    "Then we define a function time_series_to_tensor_stack that converts the time series data into PyTorch tensors by grouping data by 'time_series' and dropping the 'time_series' and 'time_step' columns, making it suitable for tensor-based machine learning models. The function is then applied to both training and test input data, and the resulting tensors and their underlying 2D numpy arrays are stored and printed for inspection.\n",
    "\n",
    "**What is a Tensor?**\n",
    "It's a multidimensional array, that generalises all types of multivariate arrays into a higher dimensions: Unifying all of the functions and things you can do to a Tensor, even if they might be of different shapes or sizes. \n",
    "\n",
    "**What is a Tensor Stack?** \n",
    "Is a tensor array (1D) made out of other Tensors. Effectively making a list of Tensors as a functional Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "c09cd569-abb6-43f5-896b-da828a0513ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONVERT THE TIME SERIES DATA IN A PROPER FORMAT FOR A TENSOR STACK\n",
    "def time_series_to_tensor(inputs):\n",
    "    time_series_data = inputs\n",
    "    time_series_tensors = []\n",
    "    # Go by each recording\n",
    "    for ts_id, recording in time_series_data.groupby('time_series'):\n",
    "        # We drop the extra columns we made earlier as they're not needed, we just needed them ordered\n",
    "        recording = recording.drop(['time_series', 'time_step'], axis=1)\n",
    "        recording = recording.values.astype(np.float32)\n",
    "        # Append recording as a tensor to the time_series list\n",
    "        time_series_tensors.append(torch.from_numpy(recording))\n",
    "    return time_series_tensors\n",
    "\n",
    "\n",
    "time_series_tensors = time_series_to_tensor(train_inputs_df_X)\n",
    "time_series_tensors_test = time_series_to_tensor(test_inputs_df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03224c5-c4db-4ee9-ae38-fd48358b3f0e",
   "metadata": {},
   "source": [
    "### Label Extraction\n",
    "Here we reshape the one-hot encoded output DataFrame into a long format using 'melt', filter for rows where the speaker is speaking, extract the speaker ID, and return a DataFrame with unique speaker IDs for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "959b92ae-429b-418d-b1ce-aba6b3707724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kikis\\AppData\\Local\\Temp\\ipykernel_31016\\1630553228.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  speaker_df['speaker'] = speaker_df['speaker'].str.extract('(\\d)').astype(int)\n",
      "C:\\Users\\kikis\\AppData\\Local\\Temp\\ipykernel_31016\\1630553228.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  speaker_df['speaker'] = speaker_df['speaker'].str.extract('(\\d)').astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Transpose the DF to just grab the speaker ID\n",
    "def extract_labels(outputs):\n",
    "    # Melt is like pivot in R, it transforms wide dfs to longh dfs\n",
    "    # So instead of 9 columns representing each speaker, we have now have an is_speaking column\n",
    "    # and a new speaker column (representing each speaker with a unique label)\n",
    "    # This is done temporarily so that we can do the next steps\n",
    "    # It pivots around the time_series and time_step columns\n",
    "    melted_df = outputs.melt(id_vars=['time_series', 'time_step'], \n",
    "                                        value_vars=[f'speaker_{i}' for i in range(1, 10)], \n",
    "                                        var_name='speaker', value_name='is_speaking')\n",
    "\n",
    "    # Now whenever a speaker is speaking, extract the corresponding speaker\n",
    "    # As an integer value\n",
    "    speaker_df = melted_df[melted_df['is_speaking'] == 1.0]\n",
    "    speaker_df['speaker'] = speaker_df['speaker'].str.extract('(\\d)').astype(int)\n",
    "\n",
    "    # We drop the unecessary columns to just have a recording column corresponding to a speaker\n",
    "    speaker_df = speaker_df.drop(columns='is_speaking')\n",
    "    speaker_df = speaker_df.drop(columns='time_step')\n",
    "    speaker_df = speaker_df.drop_duplicates()\n",
    "    speaker_df = speaker_df.reset_index(drop=True)\n",
    "    return speaker_df\n",
    "\n",
    "speaker_df = extract_labels(train_outputs_df_Y)\n",
    "speaker_df_test = extract_labels(test_outputs_df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "c466cec7-4aed-46c7-b120-219f06886566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time_series  speaker\n",
      "0              0        1\n",
      "1              1        1\n",
      "2              2        1\n",
      "3              3        1\n",
      "4              4        1\n",
      "..           ...      ...\n",
      "365          365        9\n",
      "366          366        9\n",
      "367          367        9\n",
      "368          368        9\n",
      "369          369        9\n",
      "\n",
      "[370 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(speaker_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8feb4-7e3f-4249-96ae-22ee7e7409b5",
   "metadata": {},
   "source": [
    "### Data Conversion to PyTorch Tensors for Model Input\n",
    "This code converts the preprocessed training and test input and output data into PyTorch tensors, stacks the time series tensors, extracts the speaker labels, and prints the shapes of the input and output tensors for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "0cde0e2c-9c12-4ae6-bab2-9063a1d897c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The x_inputs:  torch.Size([270, 26, 12])\n",
      "The x_inputs_test:  torch.Size([370, 29, 12])\n",
      "The y_inputs:  torch.Size([270])\n",
      "The y_inputs_test:  torch.Size([370])\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "from torch.autograd import Variable \n",
    "\n",
    "# Extract the pure values from the DFs. \n",
    "X = train_inputs_df_X.iloc[:, :].values\n",
    "Y = train_outputs_df_Y.iloc[:, :].values\n",
    "\n",
    "input_tensor = torch.stack(time_series_tensors)\n",
    "input_tensor_test = torch.stack(time_series_tensors_test)\n",
    "print(\"The x_inputs: \", input_tensor.shape)\n",
    "print(\"The x_inputs_test: \", input_tensor_test.shape)\n",
    "\n",
    "targets = torch.from_numpy(speaker_df['speaker'].values).long()\n",
    "targets_test = torch.from_numpy(speaker_df_test['speaker'].values).long()\n",
    "print(\"The y_inputs: \", targets.shape)\n",
    "print(\"The y_inputs_test: \", targets_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdd525-2fb8-4b49-83b5-6c1ce19fd1b8",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Now we define an LSTM-based neural network model for classifying time series data; specifying its architecture with LSTM and fully connected layers. We also implement the forward pass to process input sequences and produce class predictions.\n",
    "\n",
    "**Model Architecture :**\n",
    "\n",
    "The model consists of a LSTM layer that processes the input time series data, followed by a fully connected layer with 128 neurons and a ReLU activation function, and a final fully connected layer that outputs the class predictions for the 9 speakers. \n",
    "\n",
    "The LSTM layer captures temporal dependencies in the input data, while the fully connected layers refine the features for classification.\n",
    "\n",
    "\n",
    "**Forward pass implementation steps for the LSTM model :**\n",
    "\n",
    "1. Initialize hidden and cell states:\n",
    "    - Gets the batch size from the input tensor.\n",
    "    - Creates initial hidden state (h_0) and cell state (c_0) tensors filled with zeros for each layer and each sample in the batch.\n",
    "2. LSTM processing:\n",
    "    - Passes the input x and initial states (h_0, c_0) through the LSTM layer.\n",
    "    - Returns the output sequence and final hidden/cell states (hn, cn).\n",
    "3. Reshape output:\n",
    "    - Reshapes the output to ensure it's contiguous in memory and has the correct dimensions.\n",
    "    - Process final output:\n",
    "    - Selects the last output from the sequence (output[:, -1, :]).\n",
    "    - Applies ReLU activation to this last output.\n",
    "4. Fully connected layers:\n",
    "    - Passes the result through the first fully connected layer (fc_1).\n",
    "    - Applies ReLU activation again.\n",
    "    - Passes through the final fully connected layer (fc) to produce the output.\n",
    "    - Return the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "be951462-2cca-4120-a643-f801ac1c45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 2\n",
    "# Hyperparameters\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = input_size  \n",
    "        self.hidden_size = hidden_size \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)  # lstm\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size from the input tensor\n",
    "        batch_size = x.size(0)  \n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        # Propagation \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  \n",
    "        output = output.contiguous().view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        out = self.relu(output[:, -1, :])  # Apply ReLU activation to the last output\n",
    "        out = self.fc_1(out)  # First Dense layer\n",
    "        out = self.relu(out)  # ReLU activation\n",
    "        out = self.fc(out)  # Final Output layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01bac-06e9-48c6-8fa5-ca64d22f7468",
   "metadata": {},
   "source": [
    "### Define Hyper parameters\n",
    "\n",
    "We define the hyperparameters and other variables for the LSTM neural network model, including the number of classes, input size, hidden layer size, number of layers, and output size.\n",
    "\n",
    "These parameters determine the model's ability to learn and represent complex patterns in the time series data of Japanese vowel utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "b1ae2138-6749-4198-8f89-9168abceb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 9  # How many features we detecting??? \n",
    "input_size = 12  # Should be 12\n",
    "hidden_size = 80  # Size of the hidden state in the LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Size of the output (in your case, it's 1 since you have one output feature)\n",
    "num_epochs = 600\n",
    "learning_rate = 0.000179\n",
    "batch_size = 32\n",
    "# Cross Validation Parameters\n",
    "fold_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7287-6829-498a-a6f6-30b0cb734b27",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "\n",
    "Now we initialize the LSTM model with these parameters, set up the loss function, and configure the optimizer with a specific learning rate for training the model.\n",
    "\n",
    "**Loss function :** CrossEntropyLoss is used as the loss function, which is well-suited for the 9-class classification task. It measures model performance based on probability outputs, with loss increasing as predictions diverge from actual labels. \n",
    "\n",
    "**Optimizer :** Adamax, a variant of Adam optimizer, is chosen for its robustness and adaptive learning rates. It uses the infinity norm and individually adjusts learning rates for each parameter, potentially leading to faster convergence, especially with sparse gradients. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4aaf9389-d7eb-46ca-953f-91f8503bbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbaf4c-f3b9-49ee-b71d-fe5a53dd0f1d",
   "metadata": {},
   "source": [
    "Here we convert the input data and target labels to appropriate tensor types (long or float) and reshape them for training and validation, ensuring they are in the correct format for the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "e23e7742-01ed-4e7e-a91f-58923da62770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all Tensors to use long, as that's what our architecture requires\n",
    "input_data_test = input_tensor_test.long()\n",
    "targets_test = targets_test.long()\n",
    "\n",
    "#val_data = input_tensor_test.long()\n",
    "#val_targets = targets_test.long()\n",
    "\n",
    "input_data = input_tensor.float()\n",
    "targets = (targets - 1).long().view(-1)  # Convert targets to LongTensor and reshape to 1D vector (269) \n",
    "\n",
    "val_data = input_tensor_test.float()\n",
    "val_targets = (targets_test- 1).long().view(-1)  # Convert val_targets to LongTensor and reshape to 1D vector (269)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96afbba-5dca-4117-af6a-cbfe734b64cb",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "During the model training step, we feed the prepared training data into the model architecture, optimize the model parameters by minimizing a loss function over multiple epochs, and use techniques like cross-validation to improve generalization and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a561bdb-1e03-4368-94cb-2959db4c3caf",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross-validation is a technique used to evaluate the performance of a machine learning model by dividing the dataset into multiple subsets, training the model on some subsets while testing it on the remaining ones, and repeating this process to ensure the model's robustness and ability to generalize to unseen data.\n",
    "\n",
    "We set up a 3-fold cross-validation by splitting the original training dataset into two parts: one for training and one for validation, using a fixed random seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ab86dbdb-cf08-49d0-a08d-388cb6190ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "def k_split_data(input_tensor, targets, fold_num):\n",
    "    folds = []\n",
    "    kf = KFold(n_splits=fold_num, shuffle=True)\n",
    "    \n",
    "    for train_index, val_index in kf.split(input_tensor):\n",
    "        # Split your dataset\n",
    "        train_input, val_input = input_tensor[train_index], input_tensor[val_index]\n",
    "        train_targets, val_targets = targets[train_index], targets[val_index]\n",
    "        \n",
    "        # Convert to PyTorch datasets\n",
    "        train_dataset = TensorDataset(train_input, train_targets)\n",
    "        val_dataset = TensorDataset(val_input, val_targets)\n",
    "        folds.append((train_dataset, val_dataset))\n",
    "    return folds\n",
    "\n",
    "folds = k_split_data(input_data, targets, fold_num)\n",
    "full_dataset = TensorDataset(input_tensor, targets)\n",
    "val_dataset = TensorDataset(input_tensor_test, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "ec4717bf-6235-4e0c-80ee-b16f2b1c7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup up the datasets, TensorDataset fuses the inputs and outputs together for training\n",
    "#train_dataset = TensorDataset(input_tensor, targets)\n",
    "#val_dataset = TensorDataset(input_tensor_test, val_targets)\n",
    "\n",
    "# We split this dataset into folds\n",
    "#length = input_data.shape[0]\n",
    "#fold_length = int(length / fold_num)\n",
    "#remaining_length = int(length - fold_length)\n",
    "#generator1 = torch.Generator().manual_seed(42)\n",
    "\n",
    "#folds = torch.utils.data.random_split(train_dataset, [remaining_length, fold_length], generator=generator1)\n",
    "\n",
    "#print(fold_length)\n",
    "#print(length)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "9512c09b-0e71-4a4e-be05-778d1e1aa419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etestuate the model with the fold data. Accuracy and F1 testue\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_loader, index):   \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    # Check if test_loader is a DataLoader\n",
    "    if isinstance(test_loader, DataLoader):\n",
    "        # Iterate over the DataLoader\n",
    "        for inputs, targets in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                y_pred.extend(preds.cpu().numpy())\n",
    "                y_true.extend(targets.cpu().numpy())\n",
    "    else:\n",
    "        # Assuming test_loader is a tensor or a tuple of tensors\n",
    "        inputs, targets = test_loader.tensors  # This will give you the entire tensors\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_pred = preds.cpu().numpy()\n",
    "            y_true = targets.cpu().numpy()\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_true, y_pred)\n",
    "    test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Custom weighted accuracy calculation\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    class_weights = [np.sum(y_true == i) / len(y_true) for i in range(len(cm))]\n",
    "    #print(class_weights)\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    weighted_accuracy = np.sum(class_accuracies * class_weights)\n",
    "    \n",
    "    print(f\"Fold {index} - Test Accuracy: {test_accuracy:.4f}\", f\"Fold {index} - Test Weighted Accuracy: {test_accuracy:.4f}\", f\"Test weighted F1-score (macro): {test_f1:.4f}\")\n",
    "    #print(test_accuracy, test_f1)\n",
    "    return test_accuracy, weighted_accuracy, test_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "0a6f4dfd-c827-4108-90bb-f3312d3bb25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def early_stopping(val_loss, threshold=0.3): # i dont think itll ever reach this lmao probably will not use early stopping\\n    if val_loss < threshold:\\n        return True\\n    return False'"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def early_stopping(val_loss, threshold=0.3): # i dont think itll ever reach this lmao probably will not use early stopping\n",
    "    if val_loss < threshold:\n",
    "        return True\n",
    "    return False''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592d893-80c9-497d-ae5a-b39ce321890b",
   "metadata": {},
   "source": [
    "Here we implement k-fold cross-validation for training the LSTM model, where k is defined by fold_num.\n",
    "\n",
    "**Implementation steps :**\n",
    " \n",
    "1. For each fold:\n",
    "    - It separates the data into training and validation sets.\n",
    "    - Initializes a new LSTM model and optimizer.\n",
    "    - Trains the model for a specified number of epochs.\n",
    "    - Tracks training and validation losses.\n",
    "    - Saves the best model based on validation loss.\n",
    "    \n",
    "2. After training on all folds:\n",
    "    - It keeps track of the best overall model across all folds.\n",
    "    - Plots training and validation loss curves for each fold.\n",
    "    - Saves the best overall model to a file.\n",
    "    - Finally, it calculates and prints the average validation loss across all folds.\n",
    "\n",
    "  \n",
    "The fold_avg_loss list stores the best validation loss for each fold, which is used to compute the overall average performance of the model across all folds. This approach helps to assess how well the model generalizes to unseen data and provides a more robust evaluation of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "229b1055-42a2-488d-bb2c-25b1862c5d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds: 10\n",
      "Starting Fold 1/10\n",
      "Fold: 0, Epoch: 0, Train Loss: 2.19819, Val Loss: 2.20421\n",
      "Fold: 0, Epoch: 10, Train Loss: 2.19770, Val Loss: 2.20786\n",
      "Fold: 0, Epoch: 20, Train Loss: 2.19457, Val Loss: 2.20930\n",
      "Fold: 0, Epoch: 30, Train Loss: 2.18388, Val Loss: 2.19697\n",
      "Fold: 0, Epoch: 40, Train Loss: 2.11038, Val Loss: 2.09278\n",
      "Fold: 0, Epoch: 50, Train Loss: 2.01696, Val Loss: 1.96795\n",
      "Fold: 0, Epoch: 60, Train Loss: 1.88474, Val Loss: 1.83609\n",
      "Fold: 0, Epoch: 70, Train Loss: 1.75413, Val Loss: 1.70972\n",
      "Fold: 0, Epoch: 80, Train Loss: 1.65692, Val Loss: 1.61132\n",
      "Fold: 0, Epoch: 90, Train Loss: 1.58183, Val Loss: 1.57801\n",
      "Fold: 0, Epoch: 100, Train Loss: 1.51697, Val Loss: 1.52331\n",
      "Fold: 0, Epoch: 110, Train Loss: 1.47655, Val Loss: 1.49882\n",
      "Fold: 0, Epoch: 120, Train Loss: 1.44759, Val Loss: 1.48031\n",
      "Fold: 0, Epoch: 130, Train Loss: 1.41933, Val Loss: 1.48093\n",
      "Fold: 0, Epoch: 140, Train Loss: 1.40348, Val Loss: 1.45394\n",
      "Fold: 0, Epoch: 150, Train Loss: 1.38227, Val Loss: 1.46453\n",
      "Fold: 0, Epoch: 160, Train Loss: 1.37681, Val Loss: 1.44674\n",
      "Fold: 0, Epoch: 170, Train Loss: 1.36752, Val Loss: 1.41566\n",
      "Fold: 0, Epoch: 180, Train Loss: 1.35717, Val Loss: 1.39285\n",
      "Fold: 0, Epoch: 190, Train Loss: 1.35216, Val Loss: 1.36112\n",
      "Fold: 0, Epoch: 200, Train Loss: 1.33499, Val Loss: 1.39724\n",
      "Fold: 0, Epoch: 210, Train Loss: 1.31770, Val Loss: 1.31796\n",
      "Fold: 0, Epoch: 220, Train Loss: 1.31140, Val Loss: 1.31406\n",
      "Fold: 0, Epoch: 230, Train Loss: 1.31347, Val Loss: 1.33525\n",
      "Fold: 0, Epoch: 240, Train Loss: 1.27810, Val Loss: 1.29512\n",
      "Fold: 0, Epoch: 250, Train Loss: 1.27556, Val Loss: 1.26523\n",
      "Fold: 0, Epoch: 260, Train Loss: 1.24994, Val Loss: 1.26735\n",
      "Fold: 0, Epoch: 270, Train Loss: 1.24038, Val Loss: 1.24779\n",
      "Fold: 0, Epoch: 280, Train Loss: 1.22830, Val Loss: 1.20767\n",
      "Fold: 0, Epoch: 290, Train Loss: 1.21735, Val Loss: 1.18329\n",
      "Fold: 0, Epoch: 300, Train Loss: 1.18396, Val Loss: 1.16919\n",
      "Fold: 0, Epoch: 310, Train Loss: 1.16536, Val Loss: 1.11644\n",
      "Fold: 0, Epoch: 320, Train Loss: 1.15002, Val Loss: 1.09503\n",
      "Fold: 0, Epoch: 330, Train Loss: 1.11967, Val Loss: 1.07698\n",
      "Fold: 0, Epoch: 340, Train Loss: 1.09456, Val Loss: 1.05031\n",
      "Fold: 0, Epoch: 350, Train Loss: 1.07182, Val Loss: 0.99924\n",
      "Fold: 0, Epoch: 360, Train Loss: 1.05089, Val Loss: 0.95491\n",
      "Fold: 0, Epoch: 370, Train Loss: 1.03478, Val Loss: 0.94651\n",
      "Fold: 0, Epoch: 380, Train Loss: 0.99422, Val Loss: 0.93010\n",
      "Fold: 0, Epoch: 390, Train Loss: 0.97308, Val Loss: 0.87041\n",
      "Fold: 0, Epoch: 400, Train Loss: 0.92528, Val Loss: 0.81413\n",
      "Fold: 0, Epoch: 410, Train Loss: 0.91706, Val Loss: 0.77158\n",
      "Fold: 0, Epoch: 420, Train Loss: 0.86678, Val Loss: 0.79707\n",
      "Fold: 0, Epoch: 430, Train Loss: 0.83237, Val Loss: 0.79766\n",
      "Fold: 0, Epoch: 440, Train Loss: 0.79579, Val Loss: 0.73942\n",
      "Fold: 0, Epoch: 450, Train Loss: 0.76490, Val Loss: 0.78044\n",
      "Fold: 0, Epoch: 460, Train Loss: 0.71877, Val Loss: 0.66020\n",
      "Fold: 0, Epoch: 470, Train Loss: 0.66795, Val Loss: 0.68190\n",
      "Fold: 0, Epoch: 480, Train Loss: 0.63950, Val Loss: 0.68434\n",
      "Fold: 0, Epoch: 490, Train Loss: 0.61350, Val Loss: 0.71112\n",
      "Fold: 0, Epoch: 500, Train Loss: 0.58620, Val Loss: 0.67832\n",
      "Fold: 0, Epoch: 510, Train Loss: 0.52637, Val Loss: 0.66813\n",
      "Fold: 0, Epoch: 520, Train Loss: 0.49262, Val Loss: 0.62563\n",
      "Fold: 0, Epoch: 530, Train Loss: 0.46670, Val Loss: 0.63942\n",
      "Fold: 0, Epoch: 540, Train Loss: 0.43511, Val Loss: 0.60621\n",
      "Fold: 0, Epoch: 550, Train Loss: 0.40511, Val Loss: 0.66383\n",
      "Fold: 0, Epoch: 560, Train Loss: 0.37357, Val Loss: 0.69172\n",
      "Fold: 0, Epoch: 570, Train Loss: 0.34071, Val Loss: 0.54966\n",
      "Fold: 0, Epoch: 580, Train Loss: 0.30944, Val Loss: 0.64065\n",
      "Fold: 0, Epoch: 590, Train Loss: 0.28575, Val Loss: 0.53172\n",
      "Fold 0 completed. Best validation loss: 0.51381\n",
      "Fold 0 - Test Accuracy: 0.8148 Fold 0 - Test Weighted Accuracy: 0.8148 Test weighted F1-score (macro): 0.8199\n",
      "Starting Fold 2/10\n",
      "Fold: 1, Epoch: 0, Train Loss: 2.19869, Val Loss: 2.20685\n",
      "Fold: 1, Epoch: 10, Train Loss: 2.19732, Val Loss: 2.20686\n",
      "Fold: 1, Epoch: 20, Train Loss: 2.19452, Val Loss: 2.20567\n",
      "Fold: 1, Epoch: 30, Train Loss: 2.17114, Val Loss: 2.18826\n",
      "Fold: 1, Epoch: 40, Train Loss: 2.08873, Val Loss: 2.12015\n",
      "Fold: 1, Epoch: 50, Train Loss: 1.98562, Val Loss: 2.02799\n",
      "Fold: 1, Epoch: 60, Train Loss: 1.89185, Val Loss: 1.94919\n",
      "Fold: 1, Epoch: 70, Train Loss: 1.80809, Val Loss: 1.87720\n",
      "Fold: 1, Epoch: 80, Train Loss: 1.73605, Val Loss: 1.81545\n",
      "Fold: 1, Epoch: 90, Train Loss: 1.66225, Val Loss: 1.75009\n",
      "Fold: 1, Epoch: 100, Train Loss: 1.60976, Val Loss: 1.68742\n",
      "Fold: 1, Epoch: 110, Train Loss: 1.56005, Val Loss: 1.62807\n",
      "Fold: 1, Epoch: 120, Train Loss: 1.50676, Val Loss: 1.58002\n",
      "Fold: 1, Epoch: 130, Train Loss: 1.46959, Val Loss: 1.52868\n",
      "Fold: 1, Epoch: 140, Train Loss: 1.43220, Val Loss: 1.48369\n",
      "Fold: 1, Epoch: 150, Train Loss: 1.39173, Val Loss: 1.44700\n",
      "Fold: 1, Epoch: 160, Train Loss: 1.34456, Val Loss: 1.41790\n",
      "Fold: 1, Epoch: 170, Train Loss: 1.30921, Val Loss: 1.40403\n",
      "Fold: 1, Epoch: 180, Train Loss: 1.32181, Val Loss: 1.38168\n",
      "Fold: 1, Epoch: 190, Train Loss: 1.24247, Val Loss: 1.36631\n",
      "Fold: 1, Epoch: 200, Train Loss: 1.21094, Val Loss: 1.34594\n",
      "Fold: 1, Epoch: 210, Train Loss: 1.19386, Val Loss: 1.34280\n",
      "Fold: 1, Epoch: 220, Train Loss: 1.13639, Val Loss: 1.33259\n",
      "Fold: 1, Epoch: 230, Train Loss: 1.12456, Val Loss: 1.30857\n",
      "Fold: 1, Epoch: 240, Train Loss: 1.08118, Val Loss: 1.29344\n",
      "Fold: 1, Epoch: 250, Train Loss: 1.05417, Val Loss: 1.28621\n",
      "Fold: 1, Epoch: 260, Train Loss: 1.04090, Val Loss: 1.26462\n",
      "Fold: 1, Epoch: 270, Train Loss: 1.02037, Val Loss: 1.24801\n",
      "Fold: 1, Epoch: 280, Train Loss: 0.99384, Val Loss: 1.20212\n",
      "Fold: 1, Epoch: 290, Train Loss: 0.97234, Val Loss: 1.20190\n",
      "Fold: 1, Epoch: 300, Train Loss: 0.95310, Val Loss: 1.18715\n",
      "Fold: 1, Epoch: 310, Train Loss: 0.91368, Val Loss: 1.16459\n",
      "Fold: 1, Epoch: 320, Train Loss: 0.88923, Val Loss: 1.12060\n",
      "Fold: 1, Epoch: 330, Train Loss: 0.85510, Val Loss: 1.06679\n",
      "Fold: 1, Epoch: 340, Train Loss: 0.81810, Val Loss: 1.05601\n",
      "Fold: 1, Epoch: 350, Train Loss: 0.80794, Val Loss: 1.01718\n",
      "Fold: 1, Epoch: 360, Train Loss: 0.77477, Val Loss: 1.00809\n",
      "Fold: 1, Epoch: 370, Train Loss: 0.75566, Val Loss: 0.97044\n",
      "Fold: 1, Epoch: 380, Train Loss: 0.73985, Val Loss: 0.94932\n",
      "Fold: 1, Epoch: 390, Train Loss: 0.69977, Val Loss: 0.92323\n",
      "Fold: 1, Epoch: 400, Train Loss: 0.65958, Val Loss: 0.86261\n",
      "Fold: 1, Epoch: 410, Train Loss: 0.62037, Val Loss: 0.82868\n",
      "Fold: 1, Epoch: 420, Train Loss: 0.59792, Val Loss: 0.79912\n",
      "Fold: 1, Epoch: 430, Train Loss: 0.56084, Val Loss: 0.77294\n",
      "Fold: 1, Epoch: 440, Train Loss: 0.54839, Val Loss: 0.81017\n",
      "Fold: 1, Epoch: 450, Train Loss: 0.51076, Val Loss: 0.73547\n",
      "Fold: 1, Epoch: 460, Train Loss: 0.47885, Val Loss: 0.70905\n",
      "Fold: 1, Epoch: 470, Train Loss: 0.45629, Val Loss: 0.66961\n",
      "Fold: 1, Epoch: 480, Train Loss: 0.45129, Val Loss: 0.63424\n",
      "Fold: 1, Epoch: 490, Train Loss: 0.40733, Val Loss: 0.62181\n",
      "Fold: 1, Epoch: 500, Train Loss: 0.38098, Val Loss: 0.60390\n",
      "Fold: 1, Epoch: 510, Train Loss: 0.36139, Val Loss: 0.57423\n",
      "Fold: 1, Epoch: 520, Train Loss: 0.34620, Val Loss: 0.54700\n",
      "Fold: 1, Epoch: 530, Train Loss: 0.32768, Val Loss: 0.55357\n",
      "Fold: 1, Epoch: 540, Train Loss: 0.32136, Val Loss: 0.52312\n",
      "Fold: 1, Epoch: 550, Train Loss: 0.28773, Val Loss: 0.49187\n",
      "Fold: 1, Epoch: 560, Train Loss: 0.30686, Val Loss: 0.50556\n",
      "Fold: 1, Epoch: 570, Train Loss: 0.26042, Val Loss: 0.48807\n",
      "Fold: 1, Epoch: 580, Train Loss: 0.25113, Val Loss: 0.48337\n",
      "Fold: 1, Epoch: 590, Train Loss: 0.24077, Val Loss: 0.47954\n",
      "Fold 1 completed. Best validation loss: 0.44249\n",
      "Fold 1 - Test Accuracy: 0.8148 Fold 1 - Test Weighted Accuracy: 0.8148 Test weighted F1-score (macro): 0.8332\n",
      "Starting Fold 3/10\n",
      "Fold: 2, Epoch: 0, Train Loss: 2.20028, Val Loss: 2.19225\n",
      "Fold: 2, Epoch: 10, Train Loss: 2.19705, Val Loss: 2.19537\n",
      "Fold: 2, Epoch: 20, Train Loss: 2.19634, Val Loss: 2.19708\n",
      "Fold: 2, Epoch: 30, Train Loss: 2.19021, Val Loss: 2.19540\n",
      "Fold: 2, Epoch: 40, Train Loss: 2.14062, Val Loss: 2.16054\n",
      "Fold: 2, Epoch: 50, Train Loss: 2.05675, Val Loss: 2.08593\n",
      "Fold: 2, Epoch: 60, Train Loss: 1.95595, Val Loss: 1.99564\n",
      "Fold: 2, Epoch: 70, Train Loss: 1.84598, Val Loss: 1.89276\n",
      "Fold: 2, Epoch: 80, Train Loss: 1.73530, Val Loss: 1.79456\n",
      "Fold: 2, Epoch: 90, Train Loss: 1.61650, Val Loss: 1.68634\n",
      "Fold: 2, Epoch: 100, Train Loss: 1.50246, Val Loss: 1.59620\n",
      "Fold: 2, Epoch: 110, Train Loss: 1.39819, Val Loss: 1.51257\n",
      "Fold: 2, Epoch: 120, Train Loss: 1.29891, Val Loss: 1.43712\n",
      "Fold: 2, Epoch: 130, Train Loss: 1.20456, Val Loss: 1.34692\n",
      "Fold: 2, Epoch: 140, Train Loss: 1.14177, Val Loss: 1.27168\n",
      "Fold: 2, Epoch: 150, Train Loss: 1.06933, Val Loss: 1.21560\n",
      "Fold: 2, Epoch: 160, Train Loss: 0.98407, Val Loss: 1.17599\n",
      "Fold: 2, Epoch: 170, Train Loss: 0.92011, Val Loss: 1.12637\n",
      "Fold: 2, Epoch: 180, Train Loss: 0.84253, Val Loss: 1.07494\n",
      "Fold: 2, Epoch: 190, Train Loss: 0.78001, Val Loss: 1.01031\n",
      "Fold: 2, Epoch: 200, Train Loss: 0.72170, Val Loss: 0.94778\n",
      "Fold: 2, Epoch: 210, Train Loss: 0.66980, Val Loss: 0.85935\n",
      "Fold: 2, Epoch: 220, Train Loss: 0.61143, Val Loss: 0.78805\n",
      "Fold: 2, Epoch: 230, Train Loss: 0.56300, Val Loss: 0.72756\n",
      "Fold: 2, Epoch: 240, Train Loss: 0.50408, Val Loss: 0.68457\n",
      "Fold: 2, Epoch: 250, Train Loss: 0.46889, Val Loss: 0.65495\n",
      "Fold: 2, Epoch: 260, Train Loss: 0.43710, Val Loss: 0.62916\n",
      "Fold: 2, Epoch: 270, Train Loss: 0.39902, Val Loss: 0.57282\n",
      "Fold: 2, Epoch: 280, Train Loss: 0.37688, Val Loss: 0.49723\n",
      "Fold: 2, Epoch: 290, Train Loss: 0.33940, Val Loss: 0.46825\n",
      "Fold: 2, Epoch: 300, Train Loss: 0.30378, Val Loss: 0.41464\n",
      "Fold: 2, Epoch: 310, Train Loss: 0.28254, Val Loss: 0.43444\n",
      "Fold: 2, Epoch: 320, Train Loss: 0.26052, Val Loss: 0.33185\n",
      "Fold: 2, Epoch: 330, Train Loss: 0.22114, Val Loss: 0.34849\n",
      "Fold: 2, Epoch: 340, Train Loss: 0.20066, Val Loss: 0.31322\n",
      "Fold: 2, Epoch: 350, Train Loss: 0.17983, Val Loss: 0.30078\n",
      "Fold: 2, Epoch: 360, Train Loss: 0.16492, Val Loss: 0.29820\n",
      "Fold: 2, Epoch: 370, Train Loss: 0.15035, Val Loss: 0.29082\n",
      "Fold: 2, Epoch: 380, Train Loss: 0.13147, Val Loss: 0.27621\n",
      "Fold: 2, Epoch: 390, Train Loss: 0.11715, Val Loss: 0.26724\n",
      "Fold: 2, Epoch: 400, Train Loss: 0.10482, Val Loss: 0.25436\n",
      "Fold: 2, Epoch: 410, Train Loss: 0.10013, Val Loss: 0.25028\n",
      "Fold: 2, Epoch: 420, Train Loss: 0.08487, Val Loss: 0.26961\n",
      "Fold: 2, Epoch: 430, Train Loss: 0.07850, Val Loss: 0.21749\n",
      "Fold: 2, Epoch: 440, Train Loss: 0.07217, Val Loss: 0.27708\n",
      "Fold: 2, Epoch: 450, Train Loss: 0.06513, Val Loss: 0.25213\n",
      "Fold: 2, Epoch: 460, Train Loss: 0.05949, Val Loss: 0.24671\n",
      "Fold: 2, Epoch: 470, Train Loss: 0.05827, Val Loss: 0.24631\n",
      "Fold: 2, Epoch: 480, Train Loss: 0.05310, Val Loss: 0.23268\n",
      "Fold: 2, Epoch: 490, Train Loss: 0.04854, Val Loss: 0.24193\n",
      "Fold: 2, Epoch: 500, Train Loss: 0.04364, Val Loss: 0.21340\n",
      "Fold: 2, Epoch: 510, Train Loss: 0.04863, Val Loss: 0.22925\n",
      "Fold: 2, Epoch: 520, Train Loss: 0.03625, Val Loss: 0.25067\n",
      "Fold: 2, Epoch: 530, Train Loss: 0.03187, Val Loss: 0.26238\n",
      "Fold: 2, Epoch: 540, Train Loss: 0.03014, Val Loss: 0.27710\n",
      "Fold: 2, Epoch: 550, Train Loss: 0.02307, Val Loss: 0.26995\n",
      "Fold: 2, Epoch: 560, Train Loss: 0.02126, Val Loss: 0.24591\n",
      "Fold: 2, Epoch: 570, Train Loss: 0.01702, Val Loss: 0.26084\n",
      "Fold: 2, Epoch: 580, Train Loss: 0.01435, Val Loss: 0.26907\n",
      "Fold: 2, Epoch: 590, Train Loss: 0.01274, Val Loss: 0.25279\n",
      "Fold 2 completed. Best validation loss: 0.17393\n",
      "Fold 2 - Test Accuracy: 0.9630 Fold 2 - Test Weighted Accuracy: 0.9630 Test weighted F1-score (macro): 0.9624\n",
      "Starting Fold 4/10\n",
      "Fold: 3, Epoch: 0, Train Loss: 2.19791, Val Loss: 2.19969\n",
      "Fold: 3, Epoch: 10, Train Loss: 2.19707, Val Loss: 2.20202\n",
      "Fold: 3, Epoch: 20, Train Loss: 2.19414, Val Loss: 2.20295\n",
      "Fold: 3, Epoch: 30, Train Loss: 2.18709, Val Loss: 2.19680\n",
      "Fold: 3, Epoch: 40, Train Loss: 2.12902, Val Loss: 2.14155\n",
      "Fold: 3, Epoch: 50, Train Loss: 2.01060, Val Loss: 2.06258\n",
      "Fold: 3, Epoch: 60, Train Loss: 1.88912, Val Loss: 1.97611\n",
      "Fold: 3, Epoch: 70, Train Loss: 1.76060, Val Loss: 1.81969\n",
      "Fold: 3, Epoch: 80, Train Loss: 1.63045, Val Loss: 1.67800\n",
      "Fold: 3, Epoch: 90, Train Loss: 1.51468, Val Loss: 1.55182\n",
      "Fold: 3, Epoch: 100, Train Loss: 1.39663, Val Loss: 1.42401\n",
      "Fold: 3, Epoch: 110, Train Loss: 1.28937, Val Loss: 1.29087\n",
      "Fold: 3, Epoch: 120, Train Loss: 1.18327, Val Loss: 1.15343\n",
      "Fold: 3, Epoch: 130, Train Loss: 1.07692, Val Loss: 1.05725\n",
      "Fold: 3, Epoch: 140, Train Loss: 0.97788, Val Loss: 0.97939\n",
      "Fold: 3, Epoch: 150, Train Loss: 0.89179, Val Loss: 0.90850\n",
      "Fold: 3, Epoch: 160, Train Loss: 0.81260, Val Loss: 0.83960\n",
      "Fold: 3, Epoch: 170, Train Loss: 0.76785, Val Loss: 0.78100\n",
      "Fold: 3, Epoch: 180, Train Loss: 0.70717, Val Loss: 0.73571\n",
      "Fold: 3, Epoch: 190, Train Loss: 0.63918, Val Loss: 0.69230\n",
      "Fold: 3, Epoch: 200, Train Loss: 0.60024, Val Loss: 0.65459\n",
      "Fold: 3, Epoch: 210, Train Loss: 0.54599, Val Loss: 0.61352\n",
      "Fold: 3, Epoch: 220, Train Loss: 0.51509, Val Loss: 0.59046\n",
      "Fold: 3, Epoch: 230, Train Loss: 0.47043, Val Loss: 0.56862\n",
      "Fold: 3, Epoch: 240, Train Loss: 0.44454, Val Loss: 0.53598\n",
      "Fold: 3, Epoch: 250, Train Loss: 0.40461, Val Loss: 0.52785\n",
      "Fold: 3, Epoch: 260, Train Loss: 0.41273, Val Loss: 0.51380\n",
      "Fold: 3, Epoch: 270, Train Loss: 0.34628, Val Loss: 0.52071\n",
      "Fold: 3, Epoch: 280, Train Loss: 0.33322, Val Loss: 0.50393\n",
      "Fold: 3, Epoch: 290, Train Loss: 0.29805, Val Loss: 0.46572\n",
      "Fold: 3, Epoch: 300, Train Loss: 0.27028, Val Loss: 0.45712\n",
      "Fold: 3, Epoch: 310, Train Loss: 0.26184, Val Loss: 0.47261\n",
      "Fold: 3, Epoch: 320, Train Loss: 0.23227, Val Loss: 0.47128\n",
      "Fold: 3, Epoch: 330, Train Loss: 0.21216, Val Loss: 0.50266\n",
      "Fold: 3, Epoch: 340, Train Loss: 0.19955, Val Loss: 0.51248\n",
      "Fold: 3, Epoch: 350, Train Loss: 0.18298, Val Loss: 0.52555\n",
      "Fold: 3, Epoch: 360, Train Loss: 0.16719, Val Loss: 0.53054\n",
      "Fold: 3, Epoch: 370, Train Loss: 0.15838, Val Loss: 0.55712\n",
      "Fold: 3, Epoch: 380, Train Loss: 0.14426, Val Loss: 0.55835\n",
      "Fold: 3, Epoch: 390, Train Loss: 0.13206, Val Loss: 0.56674\n",
      "Fold: 3, Epoch: 400, Train Loss: 0.12773, Val Loss: 0.54677\n",
      "Fold: 3, Epoch: 410, Train Loss: 0.11549, Val Loss: 0.57259\n",
      "Fold: 3, Epoch: 420, Train Loss: 0.10699, Val Loss: 0.59141\n",
      "Fold: 3, Epoch: 430, Train Loss: 0.10125, Val Loss: 0.60320\n",
      "Fold: 3, Epoch: 440, Train Loss: 0.09679, Val Loss: 0.61788\n",
      "Fold: 3, Epoch: 450, Train Loss: 0.10515, Val Loss: 0.62166\n",
      "Fold: 3, Epoch: 460, Train Loss: 0.08836, Val Loss: 0.64050\n",
      "Fold: 3, Epoch: 470, Train Loss: 0.09391, Val Loss: 0.64730\n",
      "Fold: 3, Epoch: 480, Train Loss: 0.08053, Val Loss: 0.65673\n",
      "Fold: 3, Epoch: 490, Train Loss: 0.07639, Val Loss: 0.66953\n",
      "Fold: 3, Epoch: 500, Train Loss: 0.08811, Val Loss: 0.68225\n",
      "Fold: 3, Epoch: 510, Train Loss: 0.07044, Val Loss: 0.68425\n",
      "Fold: 3, Epoch: 520, Train Loss: 0.07323, Val Loss: 0.69238\n",
      "Fold: 3, Epoch: 530, Train Loss: 0.06457, Val Loss: 0.70959\n",
      "Fold: 3, Epoch: 540, Train Loss: 0.06234, Val Loss: 0.71947\n",
      "Fold: 3, Epoch: 550, Train Loss: 0.05902, Val Loss: 0.72355\n",
      "Fold: 3, Epoch: 560, Train Loss: 0.05993, Val Loss: 0.73377\n",
      "Fold: 3, Epoch: 570, Train Loss: 0.05599, Val Loss: 0.73635\n",
      "Fold: 3, Epoch: 580, Train Loss: 0.04808, Val Loss: 0.72485\n",
      "Fold: 3, Epoch: 590, Train Loss: 0.04655, Val Loss: 0.69857\n",
      "Fold 3 completed. Best validation loss: 0.45145\n",
      "Fold 3 - Test Accuracy: 0.8889 Fold 3 - Test Weighted Accuracy: 0.8889 Test weighted F1-score (macro): 0.8905\n",
      "Starting Fold 5/10\n",
      "Fold: 4, Epoch: 0, Train Loss: 2.19872, Val Loss: 2.20009\n",
      "Fold: 4, Epoch: 10, Train Loss: 2.19690, Val Loss: 2.20204\n",
      "Fold: 4, Epoch: 20, Train Loss: 2.19506, Val Loss: 2.20308\n",
      "Fold: 4, Epoch: 30, Train Loss: 2.18937, Val Loss: 2.20164\n",
      "Fold: 4, Epoch: 40, Train Loss: 2.13677, Val Loss: 2.17835\n",
      "Fold: 4, Epoch: 50, Train Loss: 1.95065, Val Loss: 2.03580\n",
      "Fold: 4, Epoch: 60, Train Loss: 1.73197, Val Loss: 1.86052\n",
      "Fold: 4, Epoch: 70, Train Loss: 1.51818, Val Loss: 1.71471\n",
      "Fold: 4, Epoch: 80, Train Loss: 1.36467, Val Loss: 1.57616\n",
      "Fold: 4, Epoch: 90, Train Loss: 1.21664, Val Loss: 1.45015\n",
      "Fold: 4, Epoch: 100, Train Loss: 1.10062, Val Loss: 1.34233\n",
      "Fold: 4, Epoch: 110, Train Loss: 0.97667, Val Loss: 1.24546\n",
      "Fold: 4, Epoch: 120, Train Loss: 0.87849, Val Loss: 1.14084\n",
      "Fold: 4, Epoch: 130, Train Loss: 0.78618, Val Loss: 1.03424\n",
      "Fold: 4, Epoch: 140, Train Loss: 0.69891, Val Loss: 0.96267\n",
      "Fold: 4, Epoch: 150, Train Loss: 0.63315, Val Loss: 0.91407\n",
      "Fold: 4, Epoch: 160, Train Loss: 0.57441, Val Loss: 0.88570\n",
      "Fold: 4, Epoch: 170, Train Loss: 0.50636, Val Loss: 0.84712\n",
      "Fold: 4, Epoch: 180, Train Loss: 0.44488, Val Loss: 0.79594\n",
      "Fold: 4, Epoch: 190, Train Loss: 0.39292, Val Loss: 0.75987\n",
      "Fold: 4, Epoch: 200, Train Loss: 0.35301, Val Loss: 0.78953\n",
      "Fold: 4, Epoch: 210, Train Loss: 0.31058, Val Loss: 0.80829\n",
      "Fold: 4, Epoch: 220, Train Loss: 0.27645, Val Loss: 0.81192\n",
      "Fold: 4, Epoch: 230, Train Loss: 0.24251, Val Loss: 0.76356\n",
      "Fold: 4, Epoch: 240, Train Loss: 0.22086, Val Loss: 0.76017\n",
      "Fold: 4, Epoch: 250, Train Loss: 0.19069, Val Loss: 0.72761\n",
      "Fold: 4, Epoch: 260, Train Loss: 0.17522, Val Loss: 0.71417\n",
      "Fold: 4, Epoch: 270, Train Loss: 0.16262, Val Loss: 0.71681\n",
      "Fold: 4, Epoch: 280, Train Loss: 0.14043, Val Loss: 0.71028\n",
      "Fold: 4, Epoch: 290, Train Loss: 0.13273, Val Loss: 0.70681\n",
      "Fold: 4, Epoch: 300, Train Loss: 0.12312, Val Loss: 0.65526\n",
      "Fold: 4, Epoch: 310, Train Loss: 0.11491, Val Loss: 0.68500\n",
      "Fold: 4, Epoch: 320, Train Loss: 0.10848, Val Loss: 0.67511\n",
      "Fold: 4, Epoch: 330, Train Loss: 0.09628, Val Loss: 0.67042\n",
      "Fold: 4, Epoch: 340, Train Loss: 0.11866, Val Loss: 0.65290\n",
      "Fold: 4, Epoch: 350, Train Loss: 0.09932, Val Loss: 0.72351\n",
      "Fold: 4, Epoch: 360, Train Loss: 0.08321, Val Loss: 0.72990\n",
      "Fold: 4, Epoch: 370, Train Loss: 0.08125, Val Loss: 0.71414\n",
      "Fold: 4, Epoch: 380, Train Loss: 0.07592, Val Loss: 0.70823\n",
      "Fold: 4, Epoch: 390, Train Loss: 0.07352, Val Loss: 0.71183\n",
      "Fold: 4, Epoch: 400, Train Loss: 0.07502, Val Loss: 0.70983\n",
      "Fold: 4, Epoch: 410, Train Loss: 0.07032, Val Loss: 0.67951\n",
      "Fold: 4, Epoch: 420, Train Loss: 0.06513, Val Loss: 0.72096\n",
      "Fold: 4, Epoch: 430, Train Loss: 0.06525, Val Loss: 0.69985\n",
      "Fold: 4, Epoch: 440, Train Loss: 0.06718, Val Loss: 0.67903\n",
      "Fold: 4, Epoch: 450, Train Loss: 0.05734, Val Loss: 0.67637\n",
      "Fold: 4, Epoch: 460, Train Loss: 0.05203, Val Loss: 0.67084\n",
      "Fold: 4, Epoch: 470, Train Loss: 0.05409, Val Loss: 0.64772\n",
      "Fold: 4, Epoch: 480, Train Loss: 0.05425, Val Loss: 0.64556\n",
      "Fold: 4, Epoch: 490, Train Loss: 0.04516, Val Loss: 0.65395\n",
      "Fold: 4, Epoch: 500, Train Loss: 0.04454, Val Loss: 0.63745\n",
      "Fold: 4, Epoch: 510, Train Loss: 0.04283, Val Loss: 0.64219\n",
      "Fold: 4, Epoch: 520, Train Loss: 0.04151, Val Loss: 0.64637\n",
      "Fold: 4, Epoch: 530, Train Loss: 0.03940, Val Loss: 0.64304\n",
      "Fold: 4, Epoch: 540, Train Loss: 0.03759, Val Loss: 0.65777\n",
      "Fold: 4, Epoch: 550, Train Loss: 0.03669, Val Loss: 0.65404\n",
      "Fold: 4, Epoch: 560, Train Loss: 0.04231, Val Loss: 0.63047\n",
      "Fold: 4, Epoch: 570, Train Loss: 0.03277, Val Loss: 0.66364\n",
      "Fold: 4, Epoch: 580, Train Loss: 0.03493, Val Loss: 0.59876\n",
      "Fold: 4, Epoch: 590, Train Loss: 0.03121, Val Loss: 0.62929\n",
      "Fold 4 completed. Best validation loss: 0.57603\n",
      "Fold 4 - Test Accuracy: 0.8889 Fold 4 - Test Weighted Accuracy: 0.8889 Test weighted F1-score (macro): 0.8974\n",
      "Starting Fold 6/10\n",
      "Fold: 5, Epoch: 0, Train Loss: 2.19695, Val Loss: 2.21777\n",
      "Fold: 5, Epoch: 10, Train Loss: 2.19643, Val Loss: 2.21961\n",
      "Fold: 5, Epoch: 20, Train Loss: 2.19446, Val Loss: 2.22056\n",
      "Fold: 5, Epoch: 30, Train Loss: 2.19110, Val Loss: 2.21961\n",
      "Fold: 5, Epoch: 40, Train Loss: 2.16286, Val Loss: 2.20223\n",
      "Fold: 5, Epoch: 50, Train Loss: 2.03383, Val Loss: 2.12725\n",
      "Fold: 5, Epoch: 60, Train Loss: 1.87510, Val Loss: 2.03687\n",
      "Fold: 5, Epoch: 70, Train Loss: 1.68964, Val Loss: 1.87538\n",
      "Fold: 5, Epoch: 80, Train Loss: 1.50775, Val Loss: 1.69619\n",
      "Fold: 5, Epoch: 90, Train Loss: 1.35538, Val Loss: 1.53629\n",
      "Fold: 5, Epoch: 100, Train Loss: 1.23303, Val Loss: 1.41238\n",
      "Fold: 5, Epoch: 110, Train Loss: 1.13993, Val Loss: 1.32066\n",
      "Fold: 5, Epoch: 120, Train Loss: 1.05521, Val Loss: 1.24110\n",
      "Fold: 5, Epoch: 130, Train Loss: 0.99046, Val Loss: 1.17861\n",
      "Fold: 5, Epoch: 140, Train Loss: 0.91038, Val Loss: 1.13086\n",
      "Fold: 5, Epoch: 150, Train Loss: 0.86273, Val Loss: 1.08453\n",
      "Fold: 5, Epoch: 160, Train Loss: 0.80201, Val Loss: 1.03448\n",
      "Fold: 5, Epoch: 170, Train Loss: 0.75917, Val Loss: 1.00168\n",
      "Fold: 5, Epoch: 180, Train Loss: 0.69357, Val Loss: 0.98025\n",
      "Fold: 5, Epoch: 190, Train Loss: 0.63873, Val Loss: 0.94593\n",
      "Fold: 5, Epoch: 200, Train Loss: 0.58787, Val Loss: 0.92868\n",
      "Fold: 5, Epoch: 210, Train Loss: 0.54699, Val Loss: 0.90708\n",
      "Fold: 5, Epoch: 220, Train Loss: 0.52136, Val Loss: 0.88691\n",
      "Fold: 5, Epoch: 230, Train Loss: 0.46848, Val Loss: 0.85608\n",
      "Fold: 5, Epoch: 240, Train Loss: 0.43661, Val Loss: 0.84365\n",
      "Fold: 5, Epoch: 250, Train Loss: 0.40720, Val Loss: 0.83411\n",
      "Fold: 5, Epoch: 260, Train Loss: 0.38924, Val Loss: 0.81797\n",
      "Fold: 5, Epoch: 270, Train Loss: 0.35455, Val Loss: 0.81640\n",
      "Fold: 5, Epoch: 280, Train Loss: 0.34317, Val Loss: 0.82069\n",
      "Fold: 5, Epoch: 290, Train Loss: 0.31734, Val Loss: 0.81968\n",
      "Fold: 5, Epoch: 300, Train Loss: 0.29396, Val Loss: 0.82061\n",
      "Fold: 5, Epoch: 310, Train Loss: 0.28223, Val Loss: 0.81162\n",
      "Fold: 5, Epoch: 320, Train Loss: 0.25640, Val Loss: 0.82169\n",
      "Fold: 5, Epoch: 330, Train Loss: 0.22938, Val Loss: 0.80815\n",
      "Fold: 5, Epoch: 340, Train Loss: 0.21904, Val Loss: 0.84073\n",
      "Fold: 5, Epoch: 350, Train Loss: 0.20481, Val Loss: 0.83943\n",
      "Fold: 5, Epoch: 360, Train Loss: 0.19459, Val Loss: 0.84107\n",
      "Fold: 5, Epoch: 370, Train Loss: 0.17299, Val Loss: 0.84721\n",
      "Fold: 5, Epoch: 380, Train Loss: 0.15833, Val Loss: 0.88096\n",
      "Fold: 5, Epoch: 390, Train Loss: 0.14951, Val Loss: 0.89484\n",
      "Fold: 5, Epoch: 400, Train Loss: 0.14079, Val Loss: 0.89990\n",
      "Fold: 5, Epoch: 410, Train Loss: 0.13778, Val Loss: 0.91495\n",
      "Fold: 5, Epoch: 420, Train Loss: 0.13327, Val Loss: 0.91231\n",
      "Fold: 5, Epoch: 430, Train Loss: 0.12072, Val Loss: 0.94241\n",
      "Fold: 5, Epoch: 440, Train Loss: 0.11667, Val Loss: 0.95791\n",
      "Fold: 5, Epoch: 450, Train Loss: 0.11323, Val Loss: 0.96754\n",
      "Fold: 5, Epoch: 460, Train Loss: 0.10671, Val Loss: 0.99067\n",
      "Fold: 5, Epoch: 470, Train Loss: 0.10334, Val Loss: 1.09363\n",
      "Fold: 5, Epoch: 480, Train Loss: 0.09514, Val Loss: 1.05821\n",
      "Fold: 5, Epoch: 490, Train Loss: 0.09546, Val Loss: 1.03663\n",
      "Fold: 5, Epoch: 500, Train Loss: 0.08746, Val Loss: 1.03835\n",
      "Fold: 5, Epoch: 510, Train Loss: 0.09120, Val Loss: 1.05305\n",
      "Fold: 5, Epoch: 520, Train Loss: 0.08737, Val Loss: 1.10290\n",
      "Fold: 5, Epoch: 530, Train Loss: 0.07811, Val Loss: 1.08793\n",
      "Fold: 5, Epoch: 540, Train Loss: 0.07595, Val Loss: 1.07329\n",
      "Fold: 5, Epoch: 550, Train Loss: 0.07534, Val Loss: 1.07593\n",
      "Fold: 5, Epoch: 560, Train Loss: 0.07290, Val Loss: 1.10931\n",
      "Fold: 5, Epoch: 570, Train Loss: 0.07174, Val Loss: 1.10350\n",
      "Fold: 5, Epoch: 580, Train Loss: 0.07533, Val Loss: 1.17910\n",
      "Fold: 5, Epoch: 590, Train Loss: 0.06733, Val Loss: 1.20079\n",
      "Fold 5 completed. Best validation loss: 0.79704\n",
      "Fold 5 - Test Accuracy: 0.7407 Fold 5 - Test Weighted Accuracy: 0.7407 Test weighted F1-score (macro): 0.7418\n",
      "Starting Fold 7/10\n",
      "Fold: 6, Epoch: 0, Train Loss: 2.19752, Val Loss: 2.20350\n",
      "Fold: 6, Epoch: 10, Train Loss: 2.19640, Val Loss: 2.20448\n",
      "Fold: 6, Epoch: 20, Train Loss: 2.19422, Val Loss: 2.20435\n",
      "Fold: 6, Epoch: 30, Train Loss: 2.17804, Val Loss: 2.20077\n",
      "Fold: 6, Epoch: 40, Train Loss: 2.10868, Val Loss: 2.16997\n",
      "Fold: 6, Epoch: 50, Train Loss: 2.02629, Val Loss: 2.10774\n",
      "Fold: 6, Epoch: 60, Train Loss: 1.94272, Val Loss: 2.02377\n",
      "Fold: 6, Epoch: 70, Train Loss: 1.85385, Val Loss: 1.91579\n",
      "Fold: 6, Epoch: 80, Train Loss: 1.74437, Val Loss: 1.80989\n",
      "Fold: 6, Epoch: 90, Train Loss: 1.66294, Val Loss: 1.71399\n",
      "Fold: 6, Epoch: 100, Train Loss: 1.55800, Val Loss: 1.66940\n",
      "Fold: 6, Epoch: 110, Train Loss: 1.47122, Val Loss: 1.56328\n",
      "Fold: 6, Epoch: 120, Train Loss: 1.37047, Val Loss: 1.50190\n",
      "Fold: 6, Epoch: 130, Train Loss: 1.29066, Val Loss: 1.44944\n",
      "Fold: 6, Epoch: 140, Train Loss: 1.22585, Val Loss: 1.37483\n",
      "Fold: 6, Epoch: 150, Train Loss: 1.14945, Val Loss: 1.30941\n",
      "Fold: 6, Epoch: 160, Train Loss: 1.06170, Val Loss: 1.24438\n",
      "Fold: 6, Epoch: 170, Train Loss: 0.99874, Val Loss: 1.19862\n",
      "Fold: 6, Epoch: 180, Train Loss: 0.91999, Val Loss: 1.14779\n",
      "Fold: 6, Epoch: 190, Train Loss: 0.85337, Val Loss: 1.09501\n",
      "Fold: 6, Epoch: 200, Train Loss: 0.78704, Val Loss: 1.04583\n",
      "Fold: 6, Epoch: 210, Train Loss: 0.73328, Val Loss: 0.86634\n",
      "Fold: 6, Epoch: 220, Train Loss: 0.69052, Val Loss: 0.89502\n",
      "Fold: 6, Epoch: 230, Train Loss: 0.61683, Val Loss: 0.80394\n",
      "Fold: 6, Epoch: 240, Train Loss: 0.59495, Val Loss: 0.81652\n",
      "Fold: 6, Epoch: 250, Train Loss: 0.52571, Val Loss: 0.68854\n",
      "Fold: 6, Epoch: 260, Train Loss: 0.49216, Val Loss: 0.65288\n",
      "Fold: 6, Epoch: 270, Train Loss: 0.45042, Val Loss: 0.61218\n",
      "Fold: 6, Epoch: 280, Train Loss: 0.41369, Val Loss: 0.63906\n",
      "Fold: 6, Epoch: 290, Train Loss: 0.37888, Val Loss: 0.60908\n",
      "Fold: 6, Epoch: 300, Train Loss: 0.39534, Val Loss: 0.59894\n",
      "Fold: 6, Epoch: 310, Train Loss: 0.33695, Val Loss: 0.58838\n",
      "Fold: 6, Epoch: 320, Train Loss: 0.31326, Val Loss: 0.57116\n",
      "Fold: 6, Epoch: 330, Train Loss: 0.28560, Val Loss: 0.56130\n",
      "Fold: 6, Epoch: 340, Train Loss: 0.26871, Val Loss: 0.54091\n",
      "Fold: 6, Epoch: 350, Train Loss: 0.26373, Val Loss: 0.54002\n",
      "Fold: 6, Epoch: 360, Train Loss: 0.24776, Val Loss: 0.48882\n",
      "Fold: 6, Epoch: 370, Train Loss: 0.23331, Val Loss: 0.48797\n",
      "Fold: 6, Epoch: 380, Train Loss: 0.21676, Val Loss: 0.46784\n",
      "Fold: 6, Epoch: 390, Train Loss: 0.20482, Val Loss: 0.48923\n",
      "Fold: 6, Epoch: 400, Train Loss: 0.18030, Val Loss: 0.44588\n",
      "Fold: 6, Epoch: 410, Train Loss: 0.17390, Val Loss: 0.43631\n",
      "Fold: 6, Epoch: 420, Train Loss: 0.17317, Val Loss: 0.44274\n",
      "Fold: 6, Epoch: 430, Train Loss: 0.15972, Val Loss: 0.44696\n",
      "Fold: 6, Epoch: 440, Train Loss: 0.15836, Val Loss: 0.45362\n",
      "Fold: 6, Epoch: 450, Train Loss: 0.14569, Val Loss: 0.45658\n",
      "Fold: 6, Epoch: 460, Train Loss: 0.13897, Val Loss: 0.46214\n",
      "Fold: 6, Epoch: 470, Train Loss: 0.12071, Val Loss: 0.44113\n",
      "Fold: 6, Epoch: 480, Train Loss: 0.11392, Val Loss: 0.45304\n",
      "Fold: 6, Epoch: 490, Train Loss: 0.11397, Val Loss: 0.46199\n",
      "Fold: 6, Epoch: 500, Train Loss: 0.10900, Val Loss: 0.46430\n",
      "Fold: 6, Epoch: 510, Train Loss: 0.10207, Val Loss: 0.47362\n",
      "Fold: 6, Epoch: 520, Train Loss: 0.09326, Val Loss: 0.48879\n",
      "Fold: 6, Epoch: 530, Train Loss: 0.10492, Val Loss: 0.49787\n",
      "Fold: 6, Epoch: 540, Train Loss: 0.09351, Val Loss: 0.49608\n",
      "Fold: 6, Epoch: 550, Train Loss: 0.09027, Val Loss: 0.48612\n",
      "Fold: 6, Epoch: 560, Train Loss: 0.08037, Val Loss: 0.48381\n",
      "Fold: 6, Epoch: 570, Train Loss: 0.08054, Val Loss: 0.48711\n",
      "Fold: 6, Epoch: 580, Train Loss: 0.07078, Val Loss: 0.49340\n",
      "Fold: 6, Epoch: 590, Train Loss: 0.06816, Val Loss: 0.49832\n",
      "Fold 6 completed. Best validation loss: 0.42125\n",
      "Fold 6 - Test Accuracy: 0.8889 Fold 6 - Test Weighted Accuracy: 0.8889 Test weighted F1-score (macro): 0.8841\n",
      "Starting Fold 8/10\n",
      "Fold: 7, Epoch: 0, Train Loss: 2.19508, Val Loss: 2.23455\n",
      "Fold: 7, Epoch: 10, Train Loss: 2.19474, Val Loss: 2.23442\n",
      "Fold: 7, Epoch: 20, Train Loss: 2.19242, Val Loss: 2.23283\n",
      "Fold: 7, Epoch: 30, Train Loss: 2.18379, Val Loss: 2.22241\n",
      "Fold: 7, Epoch: 40, Train Loss: 2.12964, Val Loss: 2.18166\n",
      "Fold: 7, Epoch: 50, Train Loss: 2.01191, Val Loss: 2.10667\n",
      "Fold: 7, Epoch: 60, Train Loss: 1.87618, Val Loss: 1.97071\n",
      "Fold: 7, Epoch: 70, Train Loss: 1.73928, Val Loss: 1.85025\n",
      "Fold: 7, Epoch: 80, Train Loss: 1.58857, Val Loss: 1.71565\n",
      "Fold: 7, Epoch: 90, Train Loss: 1.44993, Val Loss: 1.59837\n",
      "Fold: 7, Epoch: 100, Train Loss: 1.34284, Val Loss: 1.48414\n",
      "Fold: 7, Epoch: 110, Train Loss: 1.21219, Val Loss: 1.37505\n",
      "Fold: 7, Epoch: 120, Train Loss: 1.13264, Val Loss: 1.28964\n",
      "Fold: 7, Epoch: 130, Train Loss: 1.04182, Val Loss: 1.20229\n",
      "Fold: 7, Epoch: 140, Train Loss: 0.97076, Val Loss: 1.14215\n",
      "Fold: 7, Epoch: 150, Train Loss: 0.89501, Val Loss: 1.07065\n",
      "Fold: 7, Epoch: 160, Train Loss: 0.84068, Val Loss: 1.01270\n",
      "Fold: 7, Epoch: 170, Train Loss: 0.78485, Val Loss: 1.00203\n",
      "Fold: 7, Epoch: 180, Train Loss: 0.73797, Val Loss: 0.96771\n",
      "Fold: 7, Epoch: 190, Train Loss: 0.69061, Val Loss: 0.92415\n",
      "Fold: 7, Epoch: 200, Train Loss: 0.67078, Val Loss: 0.91410\n",
      "Fold: 7, Epoch: 210, Train Loss: 0.62089, Val Loss: 0.90852\n",
      "Fold: 7, Epoch: 220, Train Loss: 0.57733, Val Loss: 0.88282\n",
      "Fold: 7, Epoch: 230, Train Loss: 0.55332, Val Loss: 0.82539\n",
      "Fold: 7, Epoch: 240, Train Loss: 0.51355, Val Loss: 0.80846\n",
      "Fold: 7, Epoch: 250, Train Loss: 0.48177, Val Loss: 0.77851\n",
      "Fold: 7, Epoch: 260, Train Loss: 0.45672, Val Loss: 0.75341\n",
      "Fold: 7, Epoch: 270, Train Loss: 0.42284, Val Loss: 0.75735\n",
      "Fold: 7, Epoch: 280, Train Loss: 0.41004, Val Loss: 0.75780\n",
      "Fold: 7, Epoch: 290, Train Loss: 0.36723, Val Loss: 0.72524\n",
      "Fold: 7, Epoch: 300, Train Loss: 0.37097, Val Loss: 0.70327\n",
      "Fold: 7, Epoch: 310, Train Loss: 0.35004, Val Loss: 0.70872\n",
      "Fold: 7, Epoch: 320, Train Loss: 0.31838, Val Loss: 0.70502\n",
      "Fold: 7, Epoch: 330, Train Loss: 0.29502, Val Loss: 0.69654\n",
      "Fold: 7, Epoch: 340, Train Loss: 0.27709, Val Loss: 0.69037\n",
      "Fold: 7, Epoch: 350, Train Loss: 0.27067, Val Loss: 0.67864\n",
      "Fold: 7, Epoch: 360, Train Loss: 0.25129, Val Loss: 0.68925\n",
      "Fold: 7, Epoch: 370, Train Loss: 0.23536, Val Loss: 0.66616\n",
      "Fold: 7, Epoch: 380, Train Loss: 0.21745, Val Loss: 0.63432\n",
      "Fold: 7, Epoch: 390, Train Loss: 0.20040, Val Loss: 0.63055\n",
      "Fold: 7, Epoch: 400, Train Loss: 0.18833, Val Loss: 0.63873\n",
      "Fold: 7, Epoch: 410, Train Loss: 0.18441, Val Loss: 0.58357\n",
      "Fold: 7, Epoch: 420, Train Loss: 0.16795, Val Loss: 0.58755\n",
      "Fold: 7, Epoch: 430, Train Loss: 0.16148, Val Loss: 0.56597\n",
      "Fold: 7, Epoch: 440, Train Loss: 0.16582, Val Loss: 0.54300\n",
      "Fold: 7, Epoch: 450, Train Loss: 0.15025, Val Loss: 0.54601\n",
      "Fold: 7, Epoch: 460, Train Loss: 0.14095, Val Loss: 0.54495\n",
      "Fold: 7, Epoch: 470, Train Loss: 0.13822, Val Loss: 0.53564\n",
      "Fold: 7, Epoch: 480, Train Loss: 0.12937, Val Loss: 0.53342\n",
      "Fold: 7, Epoch: 490, Train Loss: 0.12143, Val Loss: 0.53172\n",
      "Fold: 7, Epoch: 500, Train Loss: 0.11740, Val Loss: 0.53700\n",
      "Fold: 7, Epoch: 510, Train Loss: 0.10976, Val Loss: 0.53439\n",
      "Fold: 7, Epoch: 520, Train Loss: 0.10809, Val Loss: 0.53929\n",
      "Fold: 7, Epoch: 530, Train Loss: 0.10593, Val Loss: 0.51770\n",
      "Fold: 7, Epoch: 540, Train Loss: 0.09282, Val Loss: 0.53484\n",
      "Fold: 7, Epoch: 550, Train Loss: 0.08836, Val Loss: 0.55080\n",
      "Fold: 7, Epoch: 560, Train Loss: 0.08693, Val Loss: 0.55812\n",
      "Fold: 7, Epoch: 570, Train Loss: 0.07769, Val Loss: 0.57783\n",
      "Fold: 7, Epoch: 580, Train Loss: 0.07401, Val Loss: 0.56033\n",
      "Fold: 7, Epoch: 590, Train Loss: 0.07063, Val Loss: 0.56744\n",
      "Fold 7 completed. Best validation loss: 0.51520\n",
      "Fold 7 - Test Accuracy: 0.8148 Fold 7 - Test Weighted Accuracy: 0.8148 Test weighted F1-score (macro): 0.7988\n",
      "Starting Fold 9/10\n",
      "Fold: 8, Epoch: 0, Train Loss: 2.19821, Val Loss: 2.20308\n",
      "Fold: 8, Epoch: 10, Train Loss: 2.19609, Val Loss: 2.20469\n",
      "Fold: 8, Epoch: 20, Train Loss: 2.19400, Val Loss: 2.20514\n",
      "Fold: 8, Epoch: 30, Train Loss: 2.17379, Val Loss: 2.19718\n",
      "Fold: 8, Epoch: 40, Train Loss: 2.08994, Val Loss: 2.12790\n",
      "Fold: 8, Epoch: 50, Train Loss: 1.99879, Val Loss: 2.03247\n",
      "Fold: 8, Epoch: 60, Train Loss: 1.89817, Val Loss: 1.92352\n",
      "Fold: 8, Epoch: 70, Train Loss: 1.77657, Val Loss: 1.81619\n",
      "Fold: 8, Epoch: 80, Train Loss: 1.66798, Val Loss: 1.72273\n",
      "Fold: 8, Epoch: 90, Train Loss: 1.55587, Val Loss: 1.66775\n",
      "Fold: 8, Epoch: 100, Train Loss: 1.45713, Val Loss: 1.57560\n",
      "Fold: 8, Epoch: 110, Train Loss: 1.35538, Val Loss: 1.50757\n",
      "Fold: 8, Epoch: 120, Train Loss: 1.27262, Val Loss: 1.49971\n",
      "Fold: 8, Epoch: 130, Train Loss: 1.16528, Val Loss: 1.39274\n",
      "Fold: 8, Epoch: 140, Train Loss: 1.08559, Val Loss: 1.34895\n",
      "Fold: 8, Epoch: 150, Train Loss: 1.00842, Val Loss: 1.29817\n",
      "Fold: 8, Epoch: 160, Train Loss: 0.94937, Val Loss: 1.27428\n",
      "Fold: 8, Epoch: 170, Train Loss: 0.88810, Val Loss: 1.25623\n",
      "Fold: 8, Epoch: 180, Train Loss: 0.81749, Val Loss: 1.22998\n",
      "Fold: 8, Epoch: 190, Train Loss: 0.76112, Val Loss: 1.21672\n",
      "Fold: 8, Epoch: 200, Train Loss: 0.73038, Val Loss: 1.14158\n",
      "Fold: 8, Epoch: 210, Train Loss: 0.68108, Val Loss: 1.16615\n",
      "Fold: 8, Epoch: 220, Train Loss: 0.63889, Val Loss: 1.09519\n",
      "Fold: 8, Epoch: 230, Train Loss: 0.59144, Val Loss: 1.13410\n",
      "Fold: 8, Epoch: 240, Train Loss: 0.57003, Val Loss: 1.07874\n",
      "Fold: 8, Epoch: 250, Train Loss: 0.55097, Val Loss: 1.03128\n",
      "Fold: 8, Epoch: 260, Train Loss: 0.49967, Val Loss: 1.01550\n",
      "Fold: 8, Epoch: 270, Train Loss: 0.46768, Val Loss: 0.96657\n",
      "Fold: 8, Epoch: 280, Train Loss: 0.45671, Val Loss: 0.96310\n",
      "Fold: 8, Epoch: 290, Train Loss: 0.42494, Val Loss: 0.98651\n",
      "Fold: 8, Epoch: 300, Train Loss: 0.41136, Val Loss: 0.91677\n",
      "Fold: 8, Epoch: 310, Train Loss: 0.38848, Val Loss: 0.96762\n",
      "Fold: 8, Epoch: 320, Train Loss: 0.36720, Val Loss: 0.86671\n",
      "Fold: 8, Epoch: 330, Train Loss: 0.34981, Val Loss: 0.87473\n",
      "Fold: 8, Epoch: 340, Train Loss: 0.34086, Val Loss: 0.87476\n",
      "Fold: 8, Epoch: 350, Train Loss: 0.31556, Val Loss: 0.84825\n",
      "Fold: 8, Epoch: 360, Train Loss: 0.30486, Val Loss: 0.86088\n",
      "Fold: 8, Epoch: 370, Train Loss: 0.29132, Val Loss: 0.78324\n",
      "Fold: 8, Epoch: 380, Train Loss: 0.27761, Val Loss: 0.82826\n",
      "Fold: 8, Epoch: 390, Train Loss: 0.26100, Val Loss: 0.74799\n",
      "Fold: 8, Epoch: 400, Train Loss: 0.25845, Val Loss: 0.91825\n",
      "Fold: 8, Epoch: 410, Train Loss: 0.24122, Val Loss: 0.73461\n",
      "Fold: 8, Epoch: 420, Train Loss: 0.22973, Val Loss: 0.71252\n",
      "Fold: 8, Epoch: 430, Train Loss: 0.22960, Val Loss: 0.71979\n",
      "Fold: 8, Epoch: 440, Train Loss: 0.21866, Val Loss: 0.71009\n",
      "Fold: 8, Epoch: 450, Train Loss: 0.20487, Val Loss: 0.69469\n",
      "Fold: 8, Epoch: 460, Train Loss: 0.19977, Val Loss: 0.67388\n",
      "Fold: 8, Epoch: 470, Train Loss: 0.19202, Val Loss: 0.94885\n",
      "Fold: 8, Epoch: 480, Train Loss: 0.18083, Val Loss: 0.66368\n",
      "Fold: 8, Epoch: 490, Train Loss: 0.18051, Val Loss: 0.63482\n",
      "Fold: 8, Epoch: 500, Train Loss: 0.16513, Val Loss: 0.62733\n",
      "Fold: 8, Epoch: 510, Train Loss: 0.16618, Val Loss: 0.62200\n",
      "Fold: 8, Epoch: 520, Train Loss: 0.16293, Val Loss: 0.60876\n",
      "Fold: 8, Epoch: 530, Train Loss: 0.14938, Val Loss: 0.61793\n",
      "Fold: 8, Epoch: 540, Train Loss: 0.15158, Val Loss: 0.61677\n",
      "Fold: 8, Epoch: 550, Train Loss: 0.15484, Val Loss: 0.82911\n",
      "Fold: 8, Epoch: 560, Train Loss: 0.13502, Val Loss: 0.57470\n",
      "Fold: 8, Epoch: 570, Train Loss: 0.13364, Val Loss: 0.54783\n",
      "Fold: 8, Epoch: 580, Train Loss: 0.12698, Val Loss: 0.54735\n",
      "Fold: 8, Epoch: 590, Train Loss: 0.12688, Val Loss: 0.55109\n",
      "Fold 8 completed. Best validation loss: 0.50461\n",
      "Fold 8 - Test Accuracy: 0.8148 Fold 8 - Test Weighted Accuracy: 0.8148 Test weighted F1-score (macro): 0.8011\n",
      "Starting Fold 10/10\n",
      "Fold: 9, Epoch: 0, Train Loss: 2.19906, Val Loss: 2.20584\n",
      "Fold: 9, Epoch: 10, Train Loss: 2.19708, Val Loss: 2.20627\n",
      "Fold: 9, Epoch: 20, Train Loss: 2.19712, Val Loss: 2.20591\n",
      "Fold: 9, Epoch: 30, Train Loss: 2.19079, Val Loss: 2.20199\n",
      "Fold: 9, Epoch: 40, Train Loss: 2.15938, Val Loss: 2.17451\n",
      "Fold: 9, Epoch: 50, Train Loss: 2.06382, Val Loss: 2.08281\n",
      "Fold: 9, Epoch: 60, Train Loss: 1.91128, Val Loss: 1.93778\n",
      "Fold: 9, Epoch: 70, Train Loss: 1.74321, Val Loss: 1.78538\n",
      "Fold: 9, Epoch: 80, Train Loss: 1.56802, Val Loss: 1.64102\n",
      "Fold: 9, Epoch: 90, Train Loss: 1.42285, Val Loss: 1.47669\n",
      "Fold: 9, Epoch: 100, Train Loss: 1.27885, Val Loss: 1.35040\n",
      "Fold: 9, Epoch: 110, Train Loss: 1.16043, Val Loss: 1.25263\n",
      "Fold: 9, Epoch: 120, Train Loss: 1.03723, Val Loss: 1.12215\n",
      "Fold: 9, Epoch: 130, Train Loss: 0.95275, Val Loss: 1.07529\n",
      "Fold: 9, Epoch: 140, Train Loss: 0.86445, Val Loss: 0.96291\n",
      "Fold: 9, Epoch: 150, Train Loss: 0.80647, Val Loss: 0.90017\n",
      "Fold: 9, Epoch: 160, Train Loss: 0.75174, Val Loss: 0.85230\n",
      "Fold: 9, Epoch: 170, Train Loss: 0.70210, Val Loss: 0.79014\n",
      "Fold: 9, Epoch: 180, Train Loss: 0.64754, Val Loss: 0.74791\n",
      "Fold: 9, Epoch: 190, Train Loss: 0.61321, Val Loss: 0.71140\n",
      "Fold: 9, Epoch: 200, Train Loss: 0.56654, Val Loss: 0.71092\n",
      "Fold: 9, Epoch: 210, Train Loss: 0.53529, Val Loss: 0.65315\n",
      "Fold: 9, Epoch: 220, Train Loss: 0.49578, Val Loss: 0.60115\n",
      "Fold: 9, Epoch: 230, Train Loss: 0.47367, Val Loss: 0.59486\n",
      "Fold: 9, Epoch: 240, Train Loss: 0.44377, Val Loss: 0.57583\n",
      "Fold: 9, Epoch: 250, Train Loss: 0.43378, Val Loss: 0.57172\n",
      "Fold: 9, Epoch: 260, Train Loss: 0.39573, Val Loss: 0.56495\n",
      "Fold: 9, Epoch: 270, Train Loss: 0.37790, Val Loss: 0.56984\n",
      "Fold: 9, Epoch: 280, Train Loss: 0.37229, Val Loss: 0.56176\n",
      "Fold: 9, Epoch: 290, Train Loss: 0.35338, Val Loss: 0.58346\n",
      "Fold: 9, Epoch: 300, Train Loss: 0.33907, Val Loss: 0.59961\n",
      "Fold: 9, Epoch: 310, Train Loss: 0.31863, Val Loss: 0.60426\n",
      "Fold: 9, Epoch: 320, Train Loss: 0.30722, Val Loss: 0.58847\n",
      "Fold: 9, Epoch: 330, Train Loss: 0.29946, Val Loss: 0.55405\n",
      "Fold: 9, Epoch: 340, Train Loss: 0.28604, Val Loss: 0.62908\n",
      "Fold: 9, Epoch: 350, Train Loss: 0.27129, Val Loss: 0.64596\n",
      "Fold: 9, Epoch: 360, Train Loss: 0.26562, Val Loss: 0.63079\n",
      "Fold: 9, Epoch: 370, Train Loss: 0.24974, Val Loss: 0.66714\n",
      "Fold: 9, Epoch: 380, Train Loss: 0.24105, Val Loss: 0.71801\n",
      "Fold: 9, Epoch: 390, Train Loss: 0.24594, Val Loss: 0.67404\n",
      "Fold: 9, Epoch: 400, Train Loss: 0.23288, Val Loss: 0.67714\n",
      "Fold: 9, Epoch: 410, Train Loss: 0.22475, Val Loss: 0.62114\n",
      "Fold: 9, Epoch: 420, Train Loss: 0.23878, Val Loss: 0.73859\n",
      "Fold: 9, Epoch: 430, Train Loss: 0.21062, Val Loss: 0.68106\n",
      "Fold: 9, Epoch: 440, Train Loss: 0.20449, Val Loss: 0.70783\n",
      "Fold: 9, Epoch: 450, Train Loss: 0.20014, Val Loss: 0.73164\n",
      "Fold: 9, Epoch: 460, Train Loss: 0.19420, Val Loss: 0.70458\n",
      "Fold: 9, Epoch: 470, Train Loss: 0.18278, Val Loss: 0.75343\n",
      "Fold: 9, Epoch: 480, Train Loss: 0.19065, Val Loss: 0.70828\n",
      "Fold: 9, Epoch: 490, Train Loss: 0.18017, Val Loss: 0.78791\n",
      "Fold: 9, Epoch: 500, Train Loss: 0.16889, Val Loss: 0.72258\n",
      "Fold: 9, Epoch: 510, Train Loss: 0.15938, Val Loss: 0.77782\n",
      "Fold: 9, Epoch: 520, Train Loss: 0.15861, Val Loss: 0.74757\n",
      "Fold: 9, Epoch: 530, Train Loss: 0.15441, Val Loss: 0.74408\n",
      "Fold: 9, Epoch: 540, Train Loss: 0.14301, Val Loss: 0.75645\n",
      "Fold: 9, Epoch: 550, Train Loss: 0.13699, Val Loss: 0.74837\n",
      "Fold: 9, Epoch: 560, Train Loss: 0.12576, Val Loss: 0.76371\n",
      "Fold: 9, Epoch: 570, Train Loss: 0.11671, Val Loss: 0.82719\n",
      "Fold: 9, Epoch: 580, Train Loss: 0.13210, Val Loss: 0.74919\n",
      "Fold: 9, Epoch: 590, Train Loss: 0.11896, Val Loss: 0.89390\n",
      "Fold 9 completed. Best validation loss: 0.55144\n",
      "Fold 9 - Test Accuracy: 0.8148 Fold 9 - Test Weighted Accuracy: 0.8148 Test weighted F1-score (macro): 0.7858\n",
      "Overall average loss: 0.4947243303060532  Average Weighted Test Accuracy: 0.8444444444444443  Average Test F1: 0.8414897653786543\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def bundle_info(run_id, fold_avg_loss, fold_accuracy_list, fold_f1_list, fold_num, folds_all_losses, best_loss, final_model):\n",
    "    # Calculate averages and append them\n",
    "    avg_loss = np.mean(fold_avg_loss)\n",
    "    avg_accuracy = np.mean(fold_accuracy_list)\n",
    "    avg_f1 = np.mean(fold_f1_list)\n",
    "    \n",
    "    #assert len(fold_avg_loss) == len(fold_accuracy_list) == len(fold_f1_list) == fold_num, \"Lists must all have the same length\"\n",
    "    results_df = pd.DataFrame({\n",
    "        'Run ID': [run_id] * fold_num,\n",
    "        'Final Model': final_model,\n",
    "        'Best Loss': best_loss,\n",
    "        'Loss': fold_avg_loss,\n",
    "        'Accuracy': fold_accuracy_list,\n",
    "        'F1': fold_f1_list,\n",
    "        'Num Classes': [num_classes] * fold_num,\n",
    "        'Input Size': [input_size] * fold_num,\n",
    "        'Hidden Size': [hidden_size] * fold_num,\n",
    "        'Num Layers': [num_layers] * fold_num,\n",
    "        'Output Size': [output_size] * fold_num,\n",
    "        'Num Epochs': [num_epochs] * fold_num,\n",
    "        'Learning Rate': [learning_rate] * fold_num,\n",
    "        'Batch Size': [batch_size] * fold_num,\n",
    "    })\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_loss = np.mean(fold_avg_loss)\n",
    "    avg_accuracy = np.mean(fold_accuracy_list)\n",
    "    avg_f1 = np.mean(fold_f1_list)\n",
    "    \n",
    "    if final_model != 1:\n",
    "        avg_row = pd.DataFrame({\n",
    "        'Run ID': run_id,\n",
    "        'Final Model': final_model,\n",
    "        'Best Loss': np.mean(best_loss),\n",
    "        'Loss': np.mean(fold_avg_loss),\n",
    "        'Accuracy': np.mean(fold_accuracy_list),\n",
    "        'F1': np.mean(fold_f1_list),\n",
    "        'Num Classes': num_classes,\n",
    "        'Input Size': input_size,\n",
    "        'Hidden Size': hidden_size,\n",
    "        'Num Layers': num_layers,\n",
    "        'Output Size': output_size,\n",
    "        'Num Epochs': num_epochs,\n",
    "        'Learning Rate': learning_rate,\n",
    "        'Batch Size': batch_size,  # Directly use batch_size without multiplying\n",
    "    }, index=['Average'])\n",
    "        \n",
    "        results_df = pd.concat([results_df, avg_row], ignore_index=False)\n",
    "    \n",
    "    # Define the CSV file path\n",
    "    csv_file = 'data/fold_results.csv'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    # Append if file exists; write header only if creating a new file\n",
    "    results_df.to_csv(csv_file, mode='a', index_label='Fold', header=not file_exists)\n",
    "\n",
    "    ## NEXT\n",
    "    # Now, create the second DataFrame for detailed fold losses\n",
    "    # Flatten the folds_all_losses into a single list for the DataFrame and create a corresponding fold index list\n",
    "    detailed_losses_df = prepare_detailed_losses_data(folds_all_losses, run_id, final_model)\n",
    "    detailed_losses_csv_file = 'data/fold_detailed_losses.csv'\n",
    "    detailed_losses_df.to_csv(detailed_losses_csv_file, mode='a', index=False, header=not os.path.exists(detailed_losses_csv_file))\n",
    "\n",
    "def generate_unique_id():\n",
    "    current_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    unique_id = str(uuid.uuid4())[:8]  # Take the first 8 characters of a UUID\n",
    "    return f\"{current_time}_{unique_id}\"\n",
    "\n",
    "def prepare_detailed_losses_data(folds_all_losses, run_id, final_model):\n",
    "    # Initialize lists to hold the processed data\n",
    "    run_ids = []\n",
    "    fold_indices = []\n",
    "    individual_losses = []\n",
    "    \n",
    "    # Iterate over each fold and its losses\n",
    "    for fold_index, losses in enumerate(folds_all_losses, start=1):\n",
    "        for loss in losses:\n",
    "            # Append data for each loss to the lists\n",
    "            run_ids.append(run_id)\n",
    "            if final_model:\n",
    "                fold_indices.append(100)\n",
    "            else:\n",
    "                fold_indices.append(fold_index)\n",
    "            individual_losses.append(loss)\n",
    "    \n",
    "    # Create a DataFrame from the lists\n",
    "    detailed_losses_df = pd.DataFrame({\n",
    "        'Run ID': run_ids,\n",
    "        'Fold Index': fold_indices,\n",
    "        'Loss': individual_losses\n",
    "    })\n",
    "    \n",
    "    return detailed_losses_df\n",
    "\n",
    "def run_training(folds, fold_num, run_id):\n",
    "    batch_size = 32\n",
    "    \n",
    "    best_overall_val_loss = float('inf')\n",
    "    best_overall_model_state = None\n",
    "    train_datasets = []\n",
    "    fold_avg_loss = []\n",
    "    fold_accuracy_list = []\n",
    "    fold_f1_list = []\n",
    "    best_losses = []\n",
    "    fold_index = 0\n",
    "    folds_all_losses = [[] for _ in range(fold_num)]\n",
    "    for train_dataset, val_dataset in folds:\n",
    "        fold_index = fold_index + 1\n",
    "\n",
    "        # Prepare training and validation datasets for the current fold\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        print(f\"Starting Fold {fold_index}/{fold_num}\")\n",
    "        \n",
    "        # Reinitialize the model and optimizer for each fold\n",
    "        model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "        optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch_input, batch_targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_input)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "            avg_train_loss = epoch_loss / len(train_loader)\n",
    "            losses.append(avg_train_loss)\n",
    "            folds_all_losses[fold_index-1].append(epoch_loss)\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_input, batch_targets in val_loader:\n",
    "                    val_outputs = model(batch_input)\n",
    "                    val_loss += criterion(val_outputs, batch_targets).item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Fold: {fold_index - 1}, Epoch: {epoch}, Train Loss: {avg_train_loss:.5f}, Val Loss: {avg_val_loss:.5f}\")\n",
    "            \n",
    "            # Save the model if it has the best validation loss so far\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "        \n",
    "        print(f\"Fold {fold_index - 1} completed. Best validation loss: {best_val_loss:.5f}\")\n",
    "        best_losses.append(best_val_loss)\n",
    "        # Check if this run produced the best overall model\n",
    "        #if best_val_loss < best_overall_val_loss:\n",
    "        #   best_overall_val_loss = best_val_loss\n",
    "        #    best_overall_model_state = best_model_state\n",
    "        \n",
    "        # Plot the training and validation loss curves for the fold\n",
    "        plt.figure()\n",
    "        plt.plot(losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training and Validation Loss - Fold {fold_index - 1}')\n",
    "        plt.legend()\n",
    "        \n",
    "        plot_filename = f'graphs/plot_{run_id}_folds_{fold_index}.png'\n",
    "        plt.savefig(plot_filename)\n",
    "        plt.close()\n",
    "        plt.show()\n",
    "        \n",
    "        fold_avg_loss.append(best_val_loss)\n",
    "        accuracy, weighted_accuracy, f1 = evaluate_model(model, val_loader, fold_index - 1)\n",
    "        fold_accuracy_list.append(accuracy)\n",
    "        fold_f1_list.append(f1)\n",
    "        torch.save(best_overall_model_state, 'models/best_model_cross_val_' + str(run_id) + '.pth')\n",
    "    \n",
    "    # Record data\n",
    "    bundle_info(run_id, fold_avg_loss, fold_accuracy_list, fold_f1_list, fold_num, folds_all_losses, best_losses, 0)\n",
    "    \n",
    "    return fold_avg_loss, fold_accuracy_list, fold_f1_list\n",
    "\n",
    "for _ in range(1, 2):\n",
    "    print(f\"Number of folds: {len(folds)}\")\n",
    "    run_id = generate_unique_id()\n",
    "    fold_losses, fold_weighted_accuracies, fold_f1_values = run_training(folds, fold_num, run_id)\n",
    "    print(f\"Overall average loss: {np.mean(fold_losses)}\", f\" Average Weighted Test Accuracy: {np.mean(fold_weighted_accuracies)}\", f\" Average Test F1: {np.mean(fold_f1_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1cef6-f168-43b6-a970-68491576c4bb",
   "metadata": {},
   "source": [
    "### Full Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e1df2-cffc-4796-8058-16182684c21c",
   "metadata": {},
   "source": [
    "Here we train the model on the entire training dataset by iterating through data batches, computing the loss, performing backpropagation, and updating the model's parameters over multiple epochs, while tracking and saving the best model state based on the validation training loss. \n",
    "\n",
    "The function also plots the training loss over epochs and saves the best model based on the lowest validation loss to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "1bae7362-dee0-46c5-8972-b95f5f48a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "final_model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "def train_full_dataset(train_dataset, model, run_id):\n",
    "    # Convert data and targets into a TensorDataset\n",
    "    #dataset = TensorDataset(data, targets)\n",
    "    \n",
    "    data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    losses = []\n",
    "\n",
    "    all_losses = [[] for _ in range(fold_num)]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_input, batch_targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(batch_input)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(data_loader)\n",
    "        losses.append(avg_train_loss)\n",
    "        all_losses[0].append(epoch_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Train Loss: {avg_train_loss:.5f}\")\n",
    "        \n",
    "        if avg_train_loss < best_val_loss: # uhm this doesnt seem to be right.. shouldnt it save best model with lowest val loss not training average\n",
    "            best_val_loss = avg_train_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(f\"Training completed. Best training loss: {best_val_loss:.5f}\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plot_filename = f'graphs/plot_{run_id}_final_model.png'\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    torch.save(best_model_state, 'models/best_model_full_model_' + str(run_id) + '.pth')\n",
    "    \n",
    "    model_average_accuracy, model_weighted_accuracy, model_weighted_f1 = evaluate_model(model, val_dataset, 1)\n",
    "    bundle_info(run_id, np.mean(losses), model_weighted_accuracy, model_weighted_f1, 1, all_losses, best_val_loss, 1)\n",
    "    \n",
    "    print(f\"Overall average loss: {np.mean(model_average_loss)}\", f\" Average Weighted Test Accuracy: {np.mean(model_weighted_accuracy)}\", f\" Average Test F1: {np.mean(model_weighted_f1)}\")\n",
    "\n",
    "#train_full_dataset(full_dataset, final_model, run_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a566e27-734c-4bec-954e-b35e3c5c0934",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Here we evaluate the performance of a trained LSTM model on both the training and validation datasets. Accuracy and F1-score metrics are calculated for both datasets; training set and testing set.\n",
    "\n",
    "This process allows for assessing how well the model performs on data it was trained on (training set) versus new, unseen data (testing set). Comparing these metrics helps identify if the model is overfitting or generalizing well to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "f08a71f8-6fc5-4f8e-9e9b-2d3022989402",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM1:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([400, 12]) from checkpoint, the shape in current model is torch.Size([320, 12]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([320, 80]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([320, 80]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([320, 80]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for fc_1.weight: copying a param with shape torch.Size([128, 100]) from checkpoint, the shape in current model is torch.Size([128, 80]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[432], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the best model state\u001b[39;00m\n\u001b[0;32m      3\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_full_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# from cross validation : best_model_cross_val.pth\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\neuralnetworks-rMpeXFAR-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM1:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([400, 12]) from checkpoint, the shape in current model is torch.Size([320, 12]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([320, 80]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([320, 80]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([400, 100]) from checkpoint, the shape in current model is torch.Size([320, 80]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([320]).\n\tsize mismatch for fc_1.weight: copying a param with shape torch.Size([128, 100]) from checkpoint, the shape in current model is torch.Size([128, 80])."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the best model state\n",
    "\n",
    "best_model_state = torch.load('best_full_model.pth') # from cross validation : best_model_cross_val.pth\n",
    "model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3028c1-919c-4af1-b07b-b1e8b27a0a31",
   "metadata": {},
   "source": [
    "### ROC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a3831-7a9e-4037-9d98-f6bec3c56c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(val_data)\n",
    "    y_pred_proba.extend(torch.softmax(val_outputs, dim=1).cpu().numpy())\n",
    "    y_true.extend(val_targets.cpu().numpy())\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "y_true = np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d864c-71dd-4b4d-8da6-b827b652b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44c5fd-ef76-4490-983d-d8929d4c4cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "             label=f'ROC curve of speaker {i+1} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Each Speaker')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plot_filename = f'graphs/roc_plot_{run_id}_final_model_for_each_speaker.png'\n",
    "plt.savefig(plot_filename)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647635a8-d07c-4fe9-832a-fb79c584cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})',\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Micro-average Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plot_filename = f'graphs/roc_plot_{run_id}_final_model_average.png'\n",
    "plt.savefig(plot_filename)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222ea1d-9129-42f2-bd6a-4e7753b19e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(val_data)\n",
    "    y_pred_proba.extend(torch.softmax(val_outputs, dim=1).cpu().numpy())\n",
    "    y_true.extend(val_targets.cpu().numpy())\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "y_true = np.array(y_true)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(num_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= num_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Compute weighted-average ROC AUC\n",
    "weighted_auc = roc_auc_score(y_true_bin, y_pred_proba, average='weighted')\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label=f'macro-average ROC curve (area = {roc_auc[\"macro\"]:.2f})',\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "             label=f'ROC curve of speaker {i+1} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curves (Weighted Average AUC = {weighted_auc:.2f})')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plot_filename = f'graphs/roc_plot_{run_id}_final_model_all_curves.png'\n",
    "plt.savefig(plot_filename)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046b7d8-c13b-487d-9315-c90ed99fc8a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "# ROC curve and ROC area for each class\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Get micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# get weighted-average ROC curve\n",
    "weights = np.sum(y_true_bin, axis=0) / len(y_true)\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))\n",
    "weighted_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "for i in range(num_classes):\n",
    "    weighted_tpr += np.interp(all_fpr, fpr[i], tpr[i]) * weights[i]\n",
    "\n",
    "fpr[\"weighted\"] = all_fpr\n",
    "tpr[\"weighted\"] = weighted_tpr\n",
    "roc_auc[\"weighted\"] = auc(fpr[\"weighted\"], tpr[\"weighted\"])\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# micro-average and weighted-average ROC curves\n",
    "ax1.plot(fpr[\"micro\"], tpr[\"micro\"], \n",
    "         label=f'Micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "ax1.plot(fpr[\"weighted\"], tpr[\"weighted\"],\n",
    "         label=f'Weighted-average ROC curve (area = {roc_auc[\"weighted\"]:.2f})',\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Micro and Weighted Average ROC Curves')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# ROC curves for each speaker\n",
    "for i in range(num_classes):\n",
    "    ax2.plot(fpr[i], tpr[i], lw=2,\n",
    "             label=f'ROC curve of speaker {i+1} (area = {roc_auc[i]:.2f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curves for Individual Speakers')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_filename = f'graphs/roc_plot_{run_id}_final_model_side_by_side.png'\n",
    "plt.savefig(plot_filename)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f2d80-42f3-46fe-9a22-c6ae7068f7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b809e1-7c6b-4feb-96fa-b1440f7476a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33fd09-2d5a-456a-85cb-e22ee73aed11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "poetry-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
