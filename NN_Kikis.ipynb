{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62cc135-4df5-4dff-85c0-3f170318b51c",
   "metadata": {},
   "source": [
    "# Japanese Vowel Speaker Recognition Project For Nueral Networks Course\n",
    "\n",
    "### Project description : \n",
    "\n",
    "The goal of this project is to develop a 9-class classifier that can accurately identify Japanese male speakers based on short spectral recordings of their vowel utterances. \n",
    "\n",
    "The task involves training the classifier using a dataset of 12-channel time series data representing vocal samples, and then evaluating its performance on a separate test set to hopefully have achieved the lowest possible misclassification rate.\n",
    "\n",
    "### Data overview :\n",
    "\n",
    "The dataset consists of 270 training and 370 test recordings of 12-channel time series data representing spectral recordings of the Japanese vowel /ae/ uttered by 9 male speakers, with each recording varying in length from 7 to 29 timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "eadea309-4544-491c-8254-9bdb6b63f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e351c-ed64-4297-83f2-8a452e34d183",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "This step involves reading the data from external sources and converting it into a format that can be used by the machine learning algorithm.\n",
    "\n",
    "Specifically, it's reading the raw data from text files provided to us, structuring it into input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "48dec52c-c79b-4ebd-98a8-719957c64380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_file(filename):\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        current_input = []\n",
    "        # Line by line we strip and split all values \n",
    "        for line in lines:\n",
    "            values = line.strip().split()\n",
    "            # If not the end of a record \n",
    "            if values and values[0] != '1.0' and values[0]!='1.00':\n",
    "                # We add the whole line (if invalid its going to be set to 0) \n",
    "                # First 12 set of values are input, the next 12 are output\n",
    "                # I only got this by looking through the original matlab file and inferencing this fact\n",
    "                # I could be wrong so maybe I'll ask the professor \n",
    "                input_values = [float(val) if val else 0 for val in values]\n",
    "                current_input.append(input_values)\n",
    "            # We're at the end\n",
    "            elif values and values[0] == '1.0' and values[0]!='1.00':\n",
    "                inputs.append(current_input)\n",
    "                current_input = []\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Read the files\n",
    "train_inputs = read_txt_file('ae.train')\n",
    "test_inputs = read_txt_file('ae.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89aa4c-0f86-4b57-9c8c-033af4974c61",
   "metadata": {},
   "source": [
    "### One-hot Encoding \n",
    "\n",
    "Here we are creating corresponding one-hot encoded output labels for both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "58f17882-7f38-4e02-b451-3d4db86fe86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs = []\n",
    "for i in range(270):\n",
    "    speaker_index = (i // 30) + 1  # Assuming 9 speakers, each with 30 time series\n",
    "    l = len(train_inputs[i])\n",
    "    teacher = np.zeros((l, 9))\n",
    "    teacher[:, speaker_index - 1] = 1  # One-hot encoding for speaker index\n",
    "    train_outputs.append(teacher)\n",
    "\n",
    "# Create teacher signals for test data\n",
    "test_outputs = []\n",
    "speaker_index = 1\n",
    "block_counter = 0\n",
    "block_lengths = [31, 35, 88, 44, 29, 24, 40, 50, 29]  # Assuming the same block lengths as in MATLAB code\n",
    "for i in range(370):\n",
    "    block_counter += 1\n",
    "    if block_counter > block_lengths[speaker_index - 1]:\n",
    "        speaker_index += 1\n",
    "        block_counter = 1\n",
    "    l = len(test_inputs[i])\n",
    "    teacher = np.zeros((l, 9))\n",
    "    teacher[:, speaker_index - 1] = 1  # One-hot encoding for speaker index\n",
    "    test_outputs.append(teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293168e-3324-42c4-95a9-eb39f4f9ef39",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "We find the maximum length of input and output sequences in both training and test sets, then pad all sequences to these maximum lengths with zeros, ensuring uniform dimensions for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "84195c87-08df-4463-8bd2-88f866566f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each recording of each input/output dataset pair, record the maximum length of a recording\n",
    "max_len_train_inputs = max(len(ts) for ts in train_inputs)\n",
    "max_len_train_outputs = max(len(ts) for ts in train_outputs)\n",
    "max_len_test_inputs = max(len(ts) for ts in test_inputs)\n",
    "max_len_test_outputs = max(len(ts) for ts in test_outputs)\n",
    "\n",
    "# Pad all recordings with 0s to reach max_len...\n",
    "train_inputs = [np.pad(ts, ((0, max_len_train_inputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in train_inputs]\n",
    "train_outputs = [np.pad(ts, ((0, max_len_train_outputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in train_outputs]\n",
    "test_inputs = [np.pad(ts, ((0, max_len_test_inputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in test_inputs]\n",
    "test_outputs = [np.pad(ts, ((0, max_len_test_outputs - len(ts)), (0, 0)), mode='constant', constant_values=0) for ts in test_outputs]\n",
    "\n",
    "# Convert to Numpy arrays for fun and easy manipulation\n",
    "train_inputs = np.array(train_inputs)\n",
    "test_inputs = np.array(test_inputs)\n",
    "train_outputs = np.array(train_outputs)\n",
    "test_outputs = np.array(test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd8fbc-6bd6-400a-a64d-1cf8feea8810",
   "metadata": {},
   "source": [
    "\n",
    "Here we inspect the dimensions of the data to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e12961b3-57fa-44e1-b248-f6a2de78360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 26, 12)\n",
      "(370, 29, 12)\n",
      "(270, 26, 9)\n",
      "(370, 29, 9)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shapes here\n",
    "## TRAINING\n",
    "# 270 -> Training Recordings -> 26 is the longest recording (time step) -> 12 is the number of features vectors \n",
    "# => 3D array of all recordings, with each of their time steps, with each of the 12 features present \n",
    "\n",
    "## TEST\n",
    "# 370 -> Test recordings -> 29 is the longest recording -> 9 is the number of output vectors (9 speakers) \n",
    "# => 3D array of all recording, with each of their time steps, with a speaker column corresponding to each timestep\n",
    "print(train_inputs.shape)\n",
    "print(test_inputs.shape)\n",
    "print(train_outputs.shape)\n",
    "print(test_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6f276-e043-482f-9812-f5e5577f4036",
   "metadata": {},
   "source": [
    "### Data dimension transformation\n",
    "\n",
    "Here we reshape the input and output data for both training and testing sets from 3D arrays into 2D pandas DataFrames, flattening the time series dimension and adding index columns for time series and time step.\n",
    "\n",
    "This creates structured DataFrames suitable for our machine learning model that expects a 2D input, while still preserving the original time series structure through multi-level indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f3f7e18f-87e7-48d5-98d1-57db5873692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Datasets\n",
    "\n",
    "def flatten_data_sets(inputs, outputs):\n",
    "    ## Flatten 'recordings' and 'time-steps' in a single dimension, while perserving the other vectors. \n",
    "    ## The -1 is just a pythonic way of telling the function to calculate the size of the flattened first dimension automatically\n",
    "    train_inputs_2d = inputs.reshape(-1, 12)  \n",
    "    \n",
    "    ## Then we create a pandas Dataframe, and label each feature \n",
    "    train_inputs_df = pd.DataFrame(train_inputs_2d, columns=[f'feature_{i}' for i in range(1, 13)])\n",
    "    \n",
    "    # Create a timeseries column that corresponds what recording each row belongs to,\n",
    "    # Where inputs.shape[0] is the first dimension (270), inputs.shape[1] is the length of each recording (26)\n",
    "    # Then using np.arrange(inputs.shape[1]) creates an array from 0 to 270 (evenly spaced indicies). \n",
    "    # Then np.repeat( ... , inputs.shape[1])\n",
    "    # Repeats for each time of shape[0] (270), that index shape[1] times. \n",
    "    # This results in a column where each index is associated with a recording\n",
    "    # lasting for the recording's timestep length. Effectively labeling what's going on\n",
    "    train_inputs_df['time_series'] = np.repeat(np.arange(inputs.shape[0]), inputs.shape[1])\n",
    "    \n",
    "    # This almost does the same thing but np.tile just copies the array (inputs.shape[1] for shape[0] times), resulting \n",
    "    # in a proper timestep series column\n",
    "    train_inputs_df['time_step'] = np.tile(np.arange(inputs.shape[1]), inputs.shape[0])\n",
    "    # This is just an easy way to ensure the columns are at the front\n",
    "    train_inputs_df_X = train_inputs_df.set_index(['time_series', 'time_step'])\n",
    "    train_inputs_df_X = train_inputs_df_X.reset_index()\n",
    "\n",
    "    # The exact same is done for the outputs, but instead of features, we're dealing with speakers\n",
    "    train_outputs_2d = outputs.reshape(-1, 9)\n",
    "    train_outputs_df = pd.DataFrame(train_outputs_2d, columns=[f'speaker_{i}' for i in range(1, 10)])\n",
    "    train_outputs_df['time_series'] = np.repeat(np.arange(outputs.shape[0]), outputs.shape[1])\n",
    "    train_outputs_df['time_step'] = np.tile(np.arange(outputs.shape[1]), outputs.shape[0])\n",
    "    train_outputs_df_Y = train_outputs_df.set_index(['time_series', 'time_step'])\n",
    "    train_outputs_df_Y = train_outputs_df_Y.reset_index()\n",
    "    return train_inputs_df_X, train_outputs_df_Y\n",
    "\n",
    "train_inputs_df_X, train_outputs_df_Y = flatten_data_sets(train_inputs, train_outputs)\n",
    "test_inputs_df_X, test_outputs_df_Y = flatten_data_sets(test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a4b2aeb5-6b11-48e0-872b-f1d0a0864ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10730, 14)\n",
      "(7020, 14)\n"
     ]
    }
   ],
   "source": [
    "# Validate shapes\n",
    "assert test_inputs_df_X.shape == (10730, 14), f\"Expected shape of (10730, 14), but got {test_inputs_df_X.shape}\"\n",
    "assert train_inputs_df_X.shape == (7020, 14), f\"Expected shape of (7020, 14), but got {train_inputs_df_X.shape}\"\n",
    "\n",
    "print(test_inputs_df_X.shape) \n",
    "print(train_inputs_df_X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1a9963cb-d5a0-4ccb-b485-025107c94093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10730, 11)\n",
      "(7020, 11)\n"
     ]
    }
   ],
   "source": [
    "assert test_outputs_df_Y.shape == (10730, 11), f\"Expected shape of (10730, 14), but got {test_outputs_df_Y.shape}\"\n",
    "assert train_outputs_df_Y.shape == (7020, 11), f\"Expected shape of (7020, 14), but got {train_outputs_df_Y.shape}\"\n",
    "\n",
    "print(test_outputs_df_Y.shape)\n",
    "print(train_outputs_df_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "27498686-5437-4ec2-8145-8fc16740166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_series</th>\n",
       "      <th>time_step</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.860936</td>\n",
       "      <td>-0.207383</td>\n",
       "      <td>0.261557</td>\n",
       "      <td>-0.214562</td>\n",
       "      <td>-0.171253</td>\n",
       "      <td>-0.118167</td>\n",
       "      <td>-0.277557</td>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.126701</td>\n",
       "      <td>-0.306756</td>\n",
       "      <td>-0.213076</td>\n",
       "      <td>0.088728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.891651</td>\n",
       "      <td>-0.193249</td>\n",
       "      <td>0.235363</td>\n",
       "      <td>-0.249118</td>\n",
       "      <td>-0.112890</td>\n",
       "      <td>-0.112238</td>\n",
       "      <td>-0.311997</td>\n",
       "      <td>-0.027122</td>\n",
       "      <td>0.171457</td>\n",
       "      <td>-0.289431</td>\n",
       "      <td>-0.247722</td>\n",
       "      <td>0.093011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.939205</td>\n",
       "      <td>-0.239664</td>\n",
       "      <td>0.258561</td>\n",
       "      <td>-0.291458</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.102034</td>\n",
       "      <td>-0.383300</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.169510</td>\n",
       "      <td>-0.314894</td>\n",
       "      <td>-0.227908</td>\n",
       "      <td>0.074638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.717517</td>\n",
       "      <td>-0.218572</td>\n",
       "      <td>0.217119</td>\n",
       "      <td>-0.228186</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>-0.137624</td>\n",
       "      <td>-0.403318</td>\n",
       "      <td>-0.009643</td>\n",
       "      <td>0.164607</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>-0.210105</td>\n",
       "      <td>0.098098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.741191</td>\n",
       "      <td>-0.279891</td>\n",
       "      <td>0.196583</td>\n",
       "      <td>-0.236377</td>\n",
       "      <td>-0.032012</td>\n",
       "      <td>-0.090612</td>\n",
       "      <td>-0.363134</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>0.124298</td>\n",
       "      <td>-0.351171</td>\n",
       "      <td>-0.216545</td>\n",
       "      <td>0.113899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_series  time_step  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0            0          0   1.860936  -0.207383   0.261557  -0.214562   \n",
       "1            0          1   1.891651  -0.193249   0.235363  -0.249118   \n",
       "2            0          2   1.939205  -0.239664   0.258561  -0.291458   \n",
       "3            0          3   1.717517  -0.218572   0.217119  -0.228186   \n",
       "4            0          4   1.741191  -0.279891   0.196583  -0.236377   \n",
       "\n",
       "   feature_5  feature_6  feature_7  feature_8  feature_9  feature_10  \\\n",
       "0  -0.171253  -0.118167  -0.277557   0.025668   0.126701   -0.306756   \n",
       "1  -0.112890  -0.112238  -0.311997  -0.027122   0.171457   -0.289431   \n",
       "2  -0.041053  -0.102034  -0.383300   0.019013   0.169510   -0.314894   \n",
       "3  -0.018608  -0.137624  -0.403318  -0.009643   0.164607   -0.323267   \n",
       "4  -0.032012  -0.090612  -0.363134  -0.012571   0.124298   -0.351171   \n",
       "\n",
       "   feature_11  feature_12  \n",
       "0   -0.213076    0.088728  \n",
       "1   -0.247722    0.093011  \n",
       "2   -0.227908    0.074638  \n",
       "3   -0.210105    0.098098  \n",
       "4   -0.216545    0.113899  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs_df_X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7706ca72-5241-4186-93c1-cddceb0042d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_series</th>\n",
       "      <th>time_step</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>speaker_2</th>\n",
       "      <th>speaker_3</th>\n",
       "      <th>speaker_4</th>\n",
       "      <th>speaker_5</th>\n",
       "      <th>speaker_6</th>\n",
       "      <th>speaker_7</th>\n",
       "      <th>speaker_8</th>\n",
       "      <th>speaker_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_series  time_step  speaker_1  speaker_2  speaker_3  speaker_4  \\\n",
       "0            0          0        1.0        0.0        0.0        0.0   \n",
       "1            0          1        1.0        0.0        0.0        0.0   \n",
       "2            0          2        1.0        0.0        0.0        0.0   \n",
       "3            0          3        1.0        0.0        0.0        0.0   \n",
       "4            0          4        1.0        0.0        0.0        0.0   \n",
       "\n",
       "   speaker_5  speaker_6  speaker_7  speaker_8  speaker_9  \n",
       "0        0.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0        0.0        0.0  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs_df_Y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "15eff7e0-4b8b-4866-802d-fe39e335e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DF for inspection\n",
    "train_inputs_df_X.to_csv('train_inputs.csv')\n",
    "train_outputs_df_Y.to_csv('train_outputs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca306e4-7ece-43a5-b936-12ff76da847b",
   "metadata": {},
   "source": [
    "### Tensor Conversion\n",
    "\n",
    "We save the training input and output DataFrames to CSV files.\n",
    "\n",
    "Then we define a function time_series_to_tensor_stack that converts the time series data into PyTorch tensors by grouping data by 'time_series' and dropping the 'time_series' and 'time_step' columns, making it suitable for tensor-based machine learning models. The function is then applied to both training and test input data, and the resulting tensors and their underlying 2D numpy arrays are stored and printed for inspection.\n",
    "\n",
    "**What is a Tensor?**\n",
    "It's a multidimensional array, that generalises all types of multivariate arrays into a higher dimensions: Unifying all of the functions and things you can do to a Tensor, even if they might be of different shapes or sizes. \n",
    "\n",
    "**What is a Tensor Stack?** \n",
    "Is a tensor array (1D) made out of other Tensors. Effectively making a list of Tensors as a functional Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c09cd569-abb6-43f5-896b-da828a0513ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONVERT THE TIME SERIES DATA IN A PROPER FORMAT FOR A TENSOR STACK\n",
    "def time_series_to_tensor(inputs):\n",
    "    time_series_data = inputs\n",
    "    time_series_tensors = []\n",
    "    # Go by each recording\n",
    "    for ts_id, recording in time_series_data.groupby('time_series'):\n",
    "        # We drop the extra columns we made earlier as they're not needed, we just needed them ordered\n",
    "        recording = recording.drop(['time_series', 'time_step'], axis=1)\n",
    "        recording = recording.values.astype(np.float32)\n",
    "        # Append recording as a tensor to the time_series list\n",
    "        time_series_tensors.append(torch.from_numpy(recording))\n",
    "    return time_series_tensors\n",
    "\n",
    "\n",
    "time_series_tensors = time_series_to_tensor(train_inputs_df_X)\n",
    "time_series_tensors_test = time_series_to_tensor(test_inputs_df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03224c5-c4db-4ee9-ae38-fd48358b3f0e",
   "metadata": {},
   "source": [
    "### Label Extraction\n",
    "Here we reshape the one-hot encoded output DataFrame into a long format using 'melt', filter for rows where the speaker is speaking, extract the speaker ID, and return a DataFrame with unique speaker IDs for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "959b92ae-429b-418d-b1ce-aba6b3707724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105835/1630553228.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  speaker_df['speaker'] = speaker_df['speaker'].str.extract('(\\d)').astype(int)\n",
      "/tmp/ipykernel_105835/1630553228.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  speaker_df['speaker'] = speaker_df['speaker'].str.extract('(\\d)').astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Transpose the DF to just grab the speaker ID\n",
    "def extract_labels(outputs):\n",
    "    # Melt is like pivot in R, it transforms wide dfs to longh dfs\n",
    "    # So instead of 9 columns representing each speaker, we have now have an is_speaking column\n",
    "    # and a new speaker column (representing each speaker with a unique label)\n",
    "    # This is done temporarily so that we can do the next steps\n",
    "    # It pivots around the time_series and time_step columns\n",
    "    melted_df = outputs.melt(id_vars=['time_series', 'time_step'], \n",
    "                                        value_vars=[f'speaker_{i}' for i in range(1, 10)], \n",
    "                                        var_name='speaker', value_name='is_speaking')\n",
    "\n",
    "    # Now whenever a speaker is speaking, extract the corresponding speaker\n",
    "    # As an integer value\n",
    "    speaker_df = melted_df[melted_df['is_speaking'] == 1.0]\n",
    "    speaker_df['speaker'] = speaker_df['speaker'].str.extract('(\\d)').astype(int)\n",
    "\n",
    "    # We drop the unecessary columns to just have a recording column corresponding to a speaker\n",
    "    speaker_df = speaker_df.drop(columns='is_speaking')\n",
    "    speaker_df = speaker_df.drop(columns='time_step')\n",
    "    speaker_df = speaker_df.drop_duplicates()\n",
    "    speaker_df = speaker_df.reset_index(drop=True)\n",
    "    return speaker_df\n",
    "\n",
    "speaker_df = extract_labels(train_outputs_df_Y)\n",
    "speaker_df_test = extract_labels(test_outputs_df_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c466cec7-4aed-46c7-b120-219f06886566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     time_series  speaker\n",
      "0              0        1\n",
      "1              1        1\n",
      "2              2        1\n",
      "3              3        1\n",
      "4              4        1\n",
      "..           ...      ...\n",
      "365          365        9\n",
      "366          366        9\n",
      "367          367        9\n",
      "368          368        9\n",
      "369          369        9\n",
      "\n",
      "[370 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(speaker_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8feb4-7e3f-4249-96ae-22ee7e7409b5",
   "metadata": {},
   "source": [
    "### Data Conversion to PyTorch Tensors for Model Input\n",
    "This code converts the preprocessed training and test input and output data into PyTorch tensors, stacks the time series tensors, extracts the speaker labels, and prints the shapes of the input and output tensors for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0cde0e2c-9c12-4ae6-bab2-9063a1d897c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The x_inputs:  torch.Size([270, 26, 12])\n",
      "The x_inputs_test:  torch.Size([370, 29, 12])\n",
      "The y_inputs:  torch.Size([270])\n",
      "The y_inputs_test:  torch.Size([370])\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "from torch.autograd import Variable \n",
    "\n",
    "# Extract the pure values from the DFs. \n",
    "X = train_inputs_df_X.iloc[:, :].values\n",
    "Y = train_outputs_df_Y.iloc[:, :].values\n",
    "\n",
    "input_tensor = torch.stack(time_series_tensors)\n",
    "input_tensor_test = torch.stack(time_series_tensors_test)\n",
    "print(\"The x_inputs: \", input_tensor.shape)\n",
    "print(\"The x_inputs_test: \", input_tensor_test.shape)\n",
    "\n",
    "targets = torch.from_numpy(speaker_df['speaker'].values).long()\n",
    "targets_test = torch.from_numpy(speaker_df_test['speaker'].values).long()\n",
    "print(\"The y_inputs: \", targets.shape)\n",
    "print(\"The y_inputs_test: \", targets_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdd525-2fb8-4b49-83b5-6c1ce19fd1b8",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Now we define an LSTM-based neural network model for classifying time series data; specifying its architecture with LSTM and fully connected layers. We also implement the forward pass to process input sequences and produce class predictions.\n",
    "\n",
    "**Model Architecture :**\n",
    "\n",
    "The model consists of a LSTM layer that processes the input time series data, followed by a fully connected layer with 128 neurons and a ReLU activation function, and a final fully connected layer that outputs the class predictions for the 9 speakers. \n",
    "\n",
    "The LSTM layer captures temporal dependencies in the input data, while the fully connected layers refine the features for classification.\n",
    "\n",
    "\n",
    "**Forward pass implementation steps for the LSTM model :**\n",
    "\n",
    "1. Initialize hidden and cell states:\n",
    "    - Gets the batch size from the input tensor.\n",
    "    - Creates initial hidden state (h_0) and cell state (c_0) tensors filled with zeros for each layer and each sample in the batch.\n",
    "2. LSTM processing:\n",
    "    - Passes the input x and initial states (h_0, c_0) through the LSTM layer.\n",
    "    - Returns the output sequence and final hidden/cell states (hn, cn).\n",
    "3. Reshape output:\n",
    "    - Reshapes the output to ensure it's contiguous in memory and has the correct dimensions.\n",
    "    - Process final output:\n",
    "    - Selects the last output from the sequence (output[:, -1, :]).\n",
    "    - Applies ReLU activation to this last output.\n",
    "4. Fully connected layers:\n",
    "    - Passes the result through the first fully connected layer (fc_1).\n",
    "    - Applies ReLU activation again.\n",
    "    - Passes through the final fully connected layer (fc) to produce the output.\n",
    "    - Return the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "be951462-2cca-4120-a643-f801ac1c45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 2\n",
    "# Hyperparameters\n",
    "\n",
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = input_size  \n",
    "        self.hidden_size = hidden_size \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)  # lstm\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)  # fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes)  # fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size from the input tensor\n",
    "        batch_size = x.size(0)  \n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        # Propagation \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  \n",
    "        output = output.contiguous().view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        out = self.relu(output[:, -1, :])  # Apply ReLU activation to the last output\n",
    "        out = self.fc_1(out)  # First Dense layer\n",
    "        out = self.relu(out)  # ReLU activation\n",
    "        out = self.fc(out)  # Final Output layer\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc01bac-06e9-48c6-8fa5-ca64d22f7468",
   "metadata": {},
   "source": [
    "### Define Hyper parameters\n",
    "\n",
    "We define the hyperparameters and other variables for the LSTM neural network model, including the number of classes, input size, hidden layer size, number of layers, and output size.\n",
    "\n",
    "These parameters determine the model's ability to learn and represent complex patterns in the time series data of Japanese vowel utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b1ae2138-6749-4198-8f89-9168abceb34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 9  # How many features we detecting??? \n",
    "input_size = 12  # Should be 12\n",
    "hidden_size = 100  # Size of the hidden state in the LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Size of the output (in your case, it's 1 since you have one output feature)\n",
    "num_epochs = 100#700\n",
    "learning_rate = 0.000179\n",
    "\n",
    "# Cross Validation Parameters\n",
    "fold_num = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b7287-6829-498a-a6f6-30b0cb734b27",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "\n",
    "Now we initialize the LSTM model with these parameters, set up the loss function, and configure the optimizer with a specific learning rate for training the model.\n",
    "\n",
    "**Loss function :** CrossEntropyLoss is used as the loss function, which is well-suited for the 9-class classification task. It measures model performance based on probability outputs, with loss increasing as predictions diverge from actual labels. \n",
    "\n",
    "**Optimizer :** Adamax, a variant of Adam optimizer, is chosen for its robustness and adaptive learning rates. It uses the infinity norm and individually adjusts learning rates for each parameter, potentially leading to faster convergence, especially with sparse gradients. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4aaf9389-d7eb-46ca-953f-91f8503bbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbaf4c-f3b9-49ee-b71d-fe5a53dd0f1d",
   "metadata": {},
   "source": [
    "Here we convert the input data and target labels to appropriate tensor types (long or float) and reshape them for training and validation, ensuring they are in the correct format for the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e23e7742-01ed-4e7e-a91f-58923da62770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all Tensors to use long, as that's what our architecture requires\n",
    "input_data_test = input_tensor_test.long()\n",
    "targets_test = targets_test.long()\n",
    "\n",
    "val_data = input_tensor_test.long()\n",
    "val_targets = targets_test.long()\n",
    "\n",
    "input_data = input_tensor.float()\n",
    "targets = (targets - 1).long().view(-1)  # Convert targets to LongTensor and reshape to 1D vector (269) \n",
    "\n",
    "val_data = input_tensor_test.float()\n",
    "val_targets = (targets_test- 1).long().view(-1)  # Convert val_targets to LongTensor and reshape to 1D vector (269)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96afbba-5dca-4117-af6a-cbfe734b64cb",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "During the model training step, we feed the prepared training data into the model architecture, optimize the model parameters by minimizing a loss function over multiple epochs, and use techniques like cross-validation to improve generalization and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a561bdb-1e03-4368-94cb-2959db4c3caf",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross-validation is a technique used to evaluate the performance of a machine learning model by dividing the dataset into multiple subsets, training the model on some subsets while testing it on the remaining ones, and repeating this process to ensure the model's robustness and ability to generalize to unseen data.\n",
    "\n",
    "We set up a 3-fold cross-validation by splitting the original training dataset into two parts: one for training and one for validation, using a fixed random seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ec4717bf-6235-4e0c-80ee-b16f2b1c7f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "# Setup up the datasets, TensorDataset fuses the inputs and outputs together for training\n",
    "train_dataset = TensorDataset(input_tensor, targets)\n",
    "val_dataset = TensorDataset(input_tensor_test, val_targets)\n",
    "\n",
    "# We split this dataset into folds\n",
    "length = input_data.shape[0]\n",
    "fold_length = int(length / fold_num)\n",
    "remaining_length = int(length - fold_length)\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "\n",
    "folds = torch.utils.data.random_split(train_dataset, [remaining_length, fold_length], generator=generator1)\n",
    "\n",
    "print(fold_length)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9512c09b-0e71-4a4e-be05-778d1e1aa419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with the fold data. Accuracy and F1 value\n",
    "def evaluate_model(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    # Evaluate on the training set ok\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_data)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(targets.cpu().numpy())\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_true, y_pred)\n",
    "    train_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Best Model - Training Accuracy: {train_accuracy:.4f}, Training F1-score (macro): {train_f1:.4f}\")\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    # Evaluate on the fold test sets\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(val_data)\n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        y_pred.extend(val_preds.cpu().numpy())\n",
    "        y_true.extend(val_targets.cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(y_true, y_pred)\n",
    "    val_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Best Model - Validation Accuracy: {val_accuracy:.4f}, Validation F1-score (macro): {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0a6f4dfd-c827-4108-90bb-f3312d3bb25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def early_stopping(val_loss, threshold=0.3): # i dont think itll ever reach this lmao probably will not use early stopping\\n    if val_loss < threshold:\\n        return True\\n    return False'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def early_stopping(val_loss, threshold=0.3): # i dont think itll ever reach this lmao probably will not use early stopping\n",
    "    if val_loss < threshold:\n",
    "        return True\n",
    "    return False''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592d893-80c9-497d-ae5a-b39ce321890b",
   "metadata": {},
   "source": [
    "Here we implement k-fold cross-validation for training the LSTM model, where k is defined by fold_num.\n",
    "\n",
    "**Implementation steps :**\n",
    " \n",
    "1. For each fold:\n",
    "    - It separates the data into training and validation sets.\n",
    "    - Initializes a new LSTM model and optimizer.\n",
    "    - Trains the model for a specified number of epochs.\n",
    "    - Tracks training and validation losses.\n",
    "    - Saves the best model based on validation loss.\n",
    "    \n",
    "2. After training on all folds:\n",
    "    - It keeps track of the best overall model across all folds.\n",
    "    - Plots training and validation loss curves for each fold.\n",
    "    - Saves the best overall model to a file.\n",
    "    - Finally, it calculates and prints the average validation loss across all folds.\n",
    "\n",
    "  \n",
    "The fold_avg list stores the best validation loss for each fold, which is used to compute the overall average performance of the model across all folds. This approach helps to assess how well the model generalizes to unseen data and provides a more robust evaluation of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "229b1055-42a2-488d-bb2c-25b1862c5d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds: 2\n",
      "[<torch.utils.data.dataset.Subset object at 0x7f9c952925b0>, <torch.utils.data.dataset.Subset object at 0x7f9c95292df0>]\n",
      "0\n",
      "1\n",
      "Starting Fold 1/3\n",
      "Fold: 1, Epoch: 0, Train Loss: 2.20054, Val Loss: 2.19697\n",
      "Fold: 1, Epoch: 10, Train Loss: 2.19777, Val Loss: 2.19712\n",
      "Fold: 1, Epoch: 20, Train Loss: 2.19657, Val Loss: 2.19721\n",
      "Fold: 1, Epoch: 30, Train Loss: 2.19471, Val Loss: 2.19723\n",
      "Fold: 1, Epoch: 40, Train Loss: 2.19211, Val Loss: 2.19701\n",
      "Fold: 1, Epoch: 50, Train Loss: 2.18812, Val Loss: 2.19594\n",
      "Fold: 1, Epoch: 60, Train Loss: 2.18082, Val Loss: 2.19133\n",
      "Fold: 1, Epoch: 70, Train Loss: 2.15331, Val Loss: 2.17491\n",
      "Fold: 1, Epoch: 80, Train Loss: 2.10469, Val Loss: 2.13802\n",
      "Fold: 1, Epoch: 90, Train Loss: 2.05479, Val Loss: 2.09247\n",
      "Fold 1 completed. Best validation loss: 2.04707\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model - Training Accuracy: 0.2185, Training F1-score (macro): 0.0822\n",
      "Best Model - Validation Accuracy: 0.3162, Validation F1-score (macro): 0.1040\n",
      "0\n",
      "1\n",
      "Starting Fold 2/3\n",
      "Fold: 2, Epoch: 0, Train Loss: 2.19950, Val Loss: 2.19721\n",
      "Fold: 2, Epoch: 10, Train Loss: 2.19748, Val Loss: 2.19587\n",
      "Fold: 2, Epoch: 20, Train Loss: 2.19178, Val Loss: 2.18787\n",
      "Fold: 2, Epoch: 30, Train Loss: 2.11646, Val Loss: 2.10691\n",
      "Fold: 2, Epoch: 40, Train Loss: 2.00705, Val Loss: 1.98911\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of folds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(folds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(folds)\n\u001b[0;32m---> 95\u001b[0m averages \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall average: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(averages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(averages)\n",
      "Cell \u001b[0;32mIn[205], line 39\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(folds, fold_num)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_input, batch_targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_targets)\n\u001b[1;32m     41\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Repos/NeuralNetworks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/NeuralNetworks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[198], line 26\u001b[0m, in \u001b[0;36mLSTM1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Propagatiooon time\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Apply ReLU activation to the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/NeuralNetworks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Repos/NeuralNetworks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/NeuralNetworks/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fold_avg = []\n",
    "def run_training(folds, fold_num):\n",
    "    batch_size = 32\n",
    "    \n",
    "    best_overall_val_loss = float('inf')\n",
    "    best_overall_model_state = None\n",
    "    train_datasets = []\n",
    "    fold_avg = []\n",
    "    \n",
    "    for val_index in range(fold_num - 1):\n",
    "        # Prepare training and validation datasets for the current fold\n",
    "        val_dataset = folds[val_index]\n",
    "        for i in range(fold_num - 1):\n",
    "            print(i)\n",
    "            if i != val_index: \n",
    "                train_datasets.append(folds[i])\n",
    "        #train_datasets = [folds[i] for i in range(fold_num) if i != val_index]\n",
    "        train_dataset = ConcatDataset(train_datasets)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        print(f\"Starting Fold {val_index + 1}/{fold_num}\")\n",
    "        \n",
    "        # Reinitialize the model and optimizer for each fold\n",
    "        model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "        optimizer = torch.optim.Adamax(model.parameters(), lr=0.000179)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch_input, batch_targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_input)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = epoch_loss / len(train_loader)\n",
    "            losses.append(avg_train_loss)\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_input, batch_targets in val_loader:\n",
    "                    val_outputs = model(batch_input)\n",
    "                    val_loss += criterion(val_outputs, batch_targets).item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Fold: {val_index + 1}, Epoch: {epoch}, Train Loss: {avg_train_loss:.5f}, Val Loss: {avg_val_loss:.5f}\")\n",
    "            \n",
    "            # Save the model if it has the best validation loss so far\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "        \n",
    "        print(f\"Fold {val_index + 1} completed. Best validation loss: {best_val_loss:.5f}\")\n",
    "        \n",
    "        # Check if this run produced the best overall model\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_overall_model_state = best_model_state\n",
    "        \n",
    "        # Plot the training and validation loss curves for the fold\n",
    "        plt.figure()\n",
    "        plt.plot(losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training and Validation Loss - Fold {val_index + 1}')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        fold_avg.append(best_val_loss)\n",
    "        evaluate_model(model)\n",
    "    \n",
    "    print(f\"Training completed. Best overall validation loss: {best_overall_val_loss:.5f}\")\n",
    "    \n",
    "    # Save the best overall model\n",
    "    torch.save(best_overall_model_state, 'best_model_cross_val.pth')\n",
    "    \n",
    "    return fold_avg\n",
    "print(f\"Number of folds: {len(folds)}\")\n",
    "print(folds)\n",
    "averages = run_training(folds, fold_num)\n",
    "print(f\"Overall average: {np.mean(averages)}\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e1df2-cffc-4796-8058-16182684c21c",
   "metadata": {},
   "source": [
    "Here we train the model on the entire training dataset by iterating through data batches, computing the loss, performing backpropagation, and updating the model's parameters over multiple epochs, while tracking and saving the best model state based on the validation training loss. \n",
    "\n",
    "The function also plots the training loss over epochs and saves the best model based on the lowest validation loss to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae7362-dee0-46c5-8972-b95f5f48a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_full_dataset(train_dataset):\n",
    "    # Convert data and targets into a TensorDataset\n",
    "    #dataset = TensorDataset(data, targets)\n",
    "    batch_size = 32\n",
    "    \n",
    "    data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = LSTM1(num_classes, input_size, hidden_size, num_layers, output_size)\n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.000179)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_input, batch_targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(batch_input)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(data_loader)\n",
    "        losses.append(avg_train_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Train Loss: {avg_train_loss:.5f}\")\n",
    "        \n",
    "        if avg_train_loss < best_val_loss: # uhm this doesnt seem to be right.. shouldnt it save best model with lowest val loss not training average\n",
    "            best_val_loss = avg_train_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(f\"Training completed. Best training loss: {best_val_loss:.5f}\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    torch.save(best_model_state, 'best_full_model.pth')\n",
    "\n",
    "train_full_dataset(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a566e27-734c-4bec-954e-b35e3c5c0934",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Here we evaluate the performance of a trained LSTM model on both the training and validation datasets. Accuracy and F1-score metrics are calculated for both datasets; training set and testing set.\n",
    "\n",
    "This process allows for assessing how well the model performs on data it was trained on (training set) versus new, unseen data (testing set). Comparing these metrics helps identify if the model is overfitting or generalizing well to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a71f8-6fc5-4f8e-9e9b-2d3022989402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the best model state\n",
    "best_model_state = torch.load('best_full_model.pth') # from cross validation : best_model_cross_val.pth\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Evaluate on the training set \n",
    "with torch.no_grad():\n",
    "    outputs = model(input_data)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    y_pred.extend(preds.cpu().numpy())\n",
    "    y_true.extend(targets.cpu().numpy())\n",
    "\n",
    "train_accuracy = accuracy_score(y_true, y_pred)\n",
    "train_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Best Model - Training Accuracy: {train_accuracy:.4f}, Training F1-score (macro): {train_f1:.4f}\")\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# evaluate on test set\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(val_data)\n",
    "    _, val_preds = torch.max(val_outputs, 1)\n",
    "    y_pred.extend(val_preds.cpu().numpy())\n",
    "    y_true.extend(val_targets.cpu().numpy())\n",
    "\n",
    "val_accuracy = accuracy_score(y_true, y_pred)\n",
    "val_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Best Model - Validation Accuracy: {val_accuracy:.4f}, Validation F1-score (macro): {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3028c1-919c-4af1-b07b-b1e8b27a0a31",
   "metadata": {},
   "source": [
    "### ROC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a3831-7a9e-4037-9d98-f6bec3c56c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "y_pred_proba = []\n",
    "y_true = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(val_data)\n",
    "    y_pred_proba.extend(torch.softmax(val_outputs, dim=1).cpu().numpy())\n",
    "    y_true.extend(val_targets.cpu().numpy())\n",
    "\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "y_true = np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d864c-71dd-4b4d-8da6-b827b652b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44c5fd-ef76-4490-983d-d8929d4c4cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "             label=f'ROC curve of speaker {i+1} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Each Speaker')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647635a8-d07c-4fe9-832a-fb79c584cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:0.2f})',\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Micro-average Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222ea1d-9129-42f2-bd6a-4e7753b19e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-kernel",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
